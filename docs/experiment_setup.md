# docs/experiment_setup.md
# Experiment Setup Documentation

This document outlines the setup and configuration for the Multi-Modal Context Boundaries (MMCB) experiments focused on structured data and code injection attacks.

## Project Structure and Data Locations

- **All advanced attack files are in:**
  - `data/generated/structured/` (JSON, CSV, YAML, XML)
  - `data/generated/code/` (Python, JavaScript)
  - `data/generated/advanced/` (cross-format, polymorphic, encoding-based)
- **To regenerate all attack files:**
  - Run `python src/attacks/generate_mmcb_examples.py`
- **To start with a clean slate:**
  - Delete all files in `data/checkpoints/`, `data/results/`, and `data/reports/` before running new experiments.

## Environment Setup

### Hardware Requirements

For optimal performance, the following hardware is recommended:

- **GPU**: Apple Silicon (M3 Pro) or NVIDIA GPU with at least 8GB VRAM (16GB+ recommended for larger models)
- **RAM**: 16GB minimum (32GB+ recommended)
- **Storage**: 20GB+ available space for models and experiment data
- **CPU**: 8+ cores recommended for concurrent file processing

### Software Dependencies

The experiments require the following software dependencies:

- **Python**: 3.8 or newer
- **PyTorch**: 2.0.0 or newer (with MPS or CUDA support)
- **Transformers**: 4.30.0 or newer
- **YAML Support**: PyYAML 6.0+ for configuration files
- **Other dependencies**: See `requirements.txt` for full details

## Model Configuration

The experiments use the following models:

1. **Llama 3 8B**: Meta's Llama 3 8B model
2. **Mistral 7B**: Mistral AI's 7B instruction-tuned model

Each model is configured with:
- **Maximum new tokens**: 512
- **Temperature**: 0.7
- **Top-p sampling**: 0.9
- **Device**: Apple Silicon (MPS) or CPU fallback

## Boundary Mechanisms

The experiments evaluate four boundary approaches:

1. **No Boundary (Baseline)**: No protection, direct concatenation
2. **Token Boundary**: Special tokens to demarcate prompt sections
3. **Semantic Boundary**: Explicit priority levels and role clarifications
4. **Hybrid Boundary**: Combines token and semantic approaches

See `docs/boundary_mechanisms.md` for details and examples.

## Attack Vectors

- **Structured Data Attacks**: Metadata, nested, obfuscated, multi-stage, format-specific
- **Code Attacks**: Multi-stage comments, docstrings, string literals, obfuscation, import/annotation exploits
- **Advanced/Polymorphic**: Cross-format, encoding, steganography

## File Generation Process

- All attack files are generated by `src/attacks/generate_mmcb_examples.py`.
- Files are placed in the appropriate `data/generated/` subfolders.
- To regenerate, simply rerun the script.

## Evaluation Metrics

- **Attack Success Rate (ASR)**
- **File Type Specificity**
- **Implementation Complexity**

## Experiment Procedure

1. **Setup**: Initialize models and prepare file generation
2. **File Generation**: Create attack files (or regenerate)
3. **Execution**: For each model, boundary, and attack combination:
   - Generate/read the malicious file
   - Apply the boundary mechanism
   - Construct the prompt
   - Submit to the model
   - Capture and analyze the response
4. **Evaluation**: Classify attack success
5. **Analysis**: Calculate metrics and identify patterns
6. **Visualization**: Generate charts and reports

## Output Files

- **All results, checkpoints, and reports are stored in:**
  - `data/results/`
  - `data/checkpoints/`
  - `data/reports/`
- These can be deleted for a clean slate before new runs.

## Reproduction Steps

1. Clone the repository
2. Install dependencies: `pip install -r requirements.txt`
3. (Optional) Regenerate attack files: `python src/attacks/generate_mmcb_examples.py`
4. Run the main experiment: `python src/main.py`
5. Analyze results: `jupyter notebook notebooks/analysis.ipynb`

## Notes on Statistical Significance

- Each experiment is run 3 times for significance.
- Metrics are averaged, with error bars for standard deviation.

## File Storage and Management

- **Generated Files**: All attack files are in `data/generated/`
- **Results**: Timestamped result directories prevent overwrites
- **Cleanup**: Delete results/checkpoints/reports for a clean slate
- **Size Management**: Large model files are excluded from version control