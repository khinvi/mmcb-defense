{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Context Boundaries: Analysis Notebook\n",
    "\n",
    "This notebook analyzes the results of experiments testing different context boundary mechanisms against multi-modal prompt injection attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Analysis\n",
    "\n",
    "Let's compare the vulnerability profiles of different models when facing the same attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Create comparison data by model\n",
    "    model_comparison = pd.pivot_table(\n",
    "        results_df,\n",
    "        values='attack_success',\n",
    "        index=['attack_type', 'attack_subtype', 'boundary'],\n",
    "        columns=['model'],\n",
    "        aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "    )\n",
    "    \n",
    "    # Display the comparison table\n",
    "    print(\"Attack Success Rate (%) by Model, Attack Type, and Boundary:\")\n",
    "    display(model_comparison.round(2))\n",
    "    \n",
    "    # Calculate model vulnerability difference\n",
    "    if len(model_comparison.columns) > 1:\n",
    "        # Calculate the absolute difference between models\n",
    "        model_diff = model_comparison.max(axis=1) - model_comparison.min(axis=1)\n",
    "        model_diff_df = pd.DataFrame(model_diff, columns=['Vulnerability Difference'])\n",
    "        \n",
    "        # Merge with attack and boundary information\n",
    "        model_diff_df = model_diff_df.reset_index()\n",
    "        \n",
    "        # Sort by the difference (largest first)\n",
    "        model_diff_df = model_diff_df.sort_values('Vulnerability Difference', ascending=False)\n",
    "        \n",
    "        # Display the top differences\n",
    "        print(\"\\nLargest Vulnerability Differences Between Models:\")\n",
    "        display(model_diff_df.head(10).round(2))\n",
    "        \n",
    "        # Plot the vulnerability differences\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Group by attack type and boundary for plotting\n",
    "        plot_data = model_diff_df.groupby(['attack_type', 'boundary'])['Vulnerability Difference'].mean().reset_index()\n",
    "        plot_data = plot_data.sort_values('Vulnerability Difference', ascending=False)\n",
    "        \n",
    "        # Create the plot\n",
    "        ax = sns.barplot(data=plot_data, x='attack_type', y='Vulnerability Difference', hue='boundary')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Model Vulnerability Differences by Attack Type and Boundary', fontsize=16)\n",
    "        plt.xlabel('Attack Type', fontsize=14)\n",
    "        plt.ylabel('Vulnerability Difference (%)', fontsize=14)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Boundary Type')\n",
    "        \n",
    "        # Add note about the plot\n",
    "        plt.figtext(\n",
    "            0.5, 0.01,\n",
    "            \"Larger values indicate greater differences in model vulnerabilities\",\n",
    "            ha='center',\n",
    "            fontsize=12\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Modal Transfer Effectiveness\n",
    "\n",
    "Let's dive deeper into analyzing how well boundaries established in text transfer to other modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None and metrics_df is not None:\n",
    "    # Extract transfer effectiveness metrics if they exist\n",
    "    transfer_metrics = metrics_df[metrics_df.get('metric_type', '') == 'transfer_effectiveness']\n",
    "    \n",
    "    if len(transfer_metrics) > 0:\n",
    "        # Plot the transfer effectiveness\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create the plot\n",
    "        ax = sns.barplot(data=transfer_metrics, x='boundary', y='value', hue='attack_type')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Cross-Modal Transfer Effectiveness by Boundary Type', fontsize=16)\n",
    "        plt.xlabel('Boundary Type', fontsize=14)\n",
    "        plt.ylabel('Transfer Effectiveness (%)', fontsize=14)\n",
    "        plt.legend(title='Attack Type')\n",
    "        \n",
    "        # Add note about the plot\n",
    "        plt.figtext(\n",
    "            0.5, 0.01,\n",
    "            \"Higher values indicate better cross-modal transfer of boundary protection\",\n",
    "            ha='center',\n",
    "            fontsize=12\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Create our own cross-modal transfer analysis\n",
    "        # Group results by modality and boundary\n",
    "        modality_mapping = {\n",
    "            'text_image': 'Visual',\n",
    "            'text_struct': 'Structured',\n",
    "            'text_code': 'Code'\n",
    "        }\n",
    "        \n",
    "        # Add modality column\n",
    "        results_df['modality'] = results_df['attack_type'].map(modality_mapping)\n",
    "        \n",
    "        # Calculate protection rates\n",
    "        protection_rates = pd.pivot_table(\n",
    "            results_df,\n",
    "            values='attack_success',\n",
    "            index=['modality'],\n",
    "            columns=['boundary'],\n",
    "            aggfunc=lambda x: 100 * (1 - np.mean(x))  # Convert to protection rate (higher is better)\n",
    "        )\n",
    "        \n",
    "        # Calculate transfer ratio (protection in modality / protection in baseline)\n",
    "        if 'none' in protection_rates.columns:\n",
    "            for boundary in protection_rates.columns:\n",
    "                if boundary != 'none':\n",
    "                    # Calculate relative improvement over no boundary\n",
    "                    protection_rates[f'{boundary}_improvement'] = protection_rates[boundary] - protection_rates['none']\n",
    "        \n",
    "        # Plot the improvement\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Extract improvement columns\n",
    "        improvement_cols = [col for col in protection_rates.columns if '_improvement' in col]\n",
    "        \n",
    "        if improvement_cols:\n",
    "            # Plot improvement by modality\n",
    "            protection_rates[improvement_cols].plot(kind='bar', ax=plt.gca())\n",
    "            \n",
    "            # Customize the plot\n",
    "            plt.title('Boundary Protection Improvement by Modality', fontsize=16)\n",
    "            plt.xlabel('Modality', fontsize=14)\n",
    "            plt.ylabel('Protection Improvement (percentage points)', fontsize=14)\n",
    "            plt.legend(title='Boundary Type')\n",
    "            \n",
    "            # Add note about the plot\n",
    "            plt.figtext(\n",
    "                0.5, 0.01,\n",
    "                \"Higher values indicate greater improvement in protection compared to no boundary\",\n",
    "                ha='center',\n",
    "                fontsize=12\n",
    "            )\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display the data\n",
    "            display(protection_rates.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings and Recommendations\n",
    "\n",
    "Based on the analysis, we can draw the following key findings and make recommendations for context boundary implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This cell would typically be filled in after running experiments and analyzing results\n",
    "\n",
    "findings = [\n",
    "    \"Finding 1: [To be filled based on experiment results]\",\n",
    "    \"Finding 2: [To be filled based on experiment results]\",\n",
    "    \"Finding 3: [To be filled based on experiment results]\",\n",
    "    \"Finding 4: [To be filled based on experiment results]\"\n",
    "]\n",
    "\n",
    "recommendations = [\n",
    "    \"Recommendation 1: [To be filled based on experiment results]\",\n",
    "    \"Recommendation 2: [To be filled based on experiment results]\",\n",
    "    \"Recommendation 3: [To be filled based on experiment results]\",\n",
    "    \"Recommendation 4: [To be filled based on experiment results]\"\n",
    "]\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i}. {finding}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, recommendation in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This analysis has explored the effectiveness of different context boundary mechanisms in protecting against multi-modal prompt injection attacks. Key insights include:\n",
    "\n",
    "1. The effectiveness of boundaries in transferring from text to other modalities\n",
    "2. The trade-offs between security and implementation complexity\n",
    "3. The patterns in which types of attacks succeed against different boundary types\n",
    "4. The vulnerability profiles of different models\n",
    "\n",
    "These findings provide valuable guidance for developing more robust security measures for LLMs in multi-modal contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "# Set up plotting styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('muted')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find the most recent results directory\n",
    "results_dirs = sorted(glob(os.path.join('..', 'data', 'results', 'experiment_*')))\n",
    "if results_dirs:\n",
    "    latest_results_dir = results_dirs[-1]\n",
    "    print(f\"Loading results from: {latest_results_dir}\")\n",
    "    \n",
    "    # Load results and metrics\n",
    "    results_path = os.path.join(latest_results_dir, 'results.csv')\n",
    "    metrics_path = os.path.join(latest_results_dir, 'metrics.csv')\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        results_df = pd.read_csv(results_path)\n",
    "        print(f\"Loaded {len(results_df)} experiment results\")\n",
    "    else:\n",
    "        print(f\"Results file not found: {results_path}\")\n",
    "        results_df = None\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        print(f\"Loaded metrics data\")\n",
    "    else:\n",
    "        print(f\"Metrics file not found: {metrics_path}\")\n",
    "        metrics_df = None\n",
    "else:\n",
    "    print(\"No experiment results found. Run experiments first.\")\n",
    "    results_df = None\n",
    "    metrics_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Results\n",
    "\n",
    "First, let's get a general overview of the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Basic statistics\n",
    "    total_experiments = len(results_df)\n",
    "    successful_attacks = results_df['attack_success'].sum()\n",
    "    success_rate = (successful_attacks / total_experiments) * 100\n",
    "    \n",
    "    print(f\"Total experiments: {total_experiments}\")\n",
    "    print(f\"Successful attacks: {successful_attacks} ({success_rate:.2f}%)\")\n",
    "    \n",
    "    # Success rate by model\n",
    "    model_success = results_df.groupby('model')['attack_success'].mean() * 100\n",
    "    print(\"\\nSuccess rate by model:\")\n",
    "    print(model_success)\n",
    "    \n",
    "    # Success rate by boundary type\n",
    "    boundary_success = results_df.groupby('boundary')['attack_success'].mean() * 100\n",
    "    print(\"\\nSuccess rate by boundary type:\")\n",
    "    print(boundary_success)\n",
    "    \n",
    "    # Success rate by attack type\n",
    "    attack_success = results_df.groupby('attack_type')['attack_success'].mean() * 100\n",
    "    print(\"\\nSuccess rate by attack type:\")\n",
    "    print(attack_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Boundary Effectiveness Comparison\n",
    "\n",
    "Let's analyze the effectiveness of different boundary mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Create a pivot table for comprehensive comparison\n",
    "    pivot_df = pd.pivot_table(\n",
    "        results_df,\n",
    "        values='attack_success',\n",
    "        index=['model', 'attack_type'],\n",
    "        columns=['boundary'],\n",
    "        aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "    )\n",
    "    \n",
    "    # Fill any missing values\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "    \n",
    "    # Display the pivot table\n",
    "    print(\"Attack Success Rate (%) by Model, Attack Type, and Boundary Mechanism:\")\n",
    "    display(pivot_df.round(2))\n",
    "    \n",
    "    # Calculate the reduction in success rate compared to no boundary\n",
    "    if 'none' in pivot_df.columns:\n",
    "        for boundary in pivot_df.columns:\n",
    "            if boundary != 'none':\n",
    "                pivot_df[f'{boundary}_reduction'] = pivot_df['none'] - pivot_df[boundary]\n",
    "        \n",
    "        # Display the reduction\n",
    "        reduction_cols = [col for col in pivot_df.columns if '_reduction' in col]\n",
    "        if reduction_cols:\n",
    "            print(\"\\nReduction in Attack Success Rate (percentage points) compared to No Boundary:\")\n",
    "            display(pivot_df[reduction_cols].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Modal Transfer Effectiveness\n",
    "\n",
    "Let's analyze how well boundaries in one modality transfer to other modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Group by modality and boundary type to see transfer effectiveness\n",
    "    modal_groups = {\n",
    "        'text_image': 'Visual Modality',\n",
    "        'text_struct': 'Structured Data Modality',\n",
    "        'text_code': 'Code Modality'\n",
    "    }\n",
    "    \n",
    "    # Create figure for visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot success rates by modality and boundary\n",
    "    modal_data = []\n",
    "    \n",
    "    for attack_type, modal_name in modal_groups.items():\n",
    "        modal_df = results_df[results_df['attack_type'] == attack_type]\n",
    "        \n",
    "        if len(modal_df) > 0:\n",
    "            boundary_rates = modal_df.groupby('boundary')['attack_success'].mean() * 100\n",
    "            \n",
    "            for boundary, rate in boundary_rates.items():\n",
    "                modal_data.append({\n",
    "                    'Modality': modal_name,\n",
    "                    'Boundary': boundary,\n",
    "                    'Attack Success Rate (%)': rate\n",
    "                })\n",
    "    \n",
    "    modal_df = pd.DataFrame(modal_data)\n",
    "    \n",
    "    if len(modal_df) > 0:\n",
    "        # Plot the data\n",
    "        ax = sns.barplot(data=modal_df, x='Modality', y='Attack Success Rate (%)', hue='Boundary')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Boundary Effectiveness Across Modalities', fontsize=16)\n",
    "        plt.xlabel('Modality', fontsize=14)\n",
    "        plt.ylabel('Attack Success Rate (%)', fontsize=14)\n",
    "        plt.legend(title='Boundary Type', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add note that lower is better\n",
    "        plt.figtext(0.5, 0.01, \"Lower success rate indicates better protection\", ha='center', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate cross-modal transfer effectiveness\n",
    "        if 'none' in modal_df['Boundary'].unique():\n",
    "            pivot = pd.pivot_table(\n",
    "                modal_df,\n",
    "                values='Attack Success Rate (%)',\n",
    "                index=['Modality'],\n",
    "                columns=['Boundary']\n",
    "            )\n",
    "            \n",
    "            # Calculate relative reduction in success rate\n",
    "            for boundary in pivot.columns:\n",
    "                if boundary != 'none':\n",
    "                    pivot[f'{boundary}_effectiveness'] = 100 * (1 - pivot[boundary] / pivot['none'])\n",
    "            \n",
    "            effectiveness_cols = [col for col in pivot.columns if '_effectiveness' in col]\n",
    "            if effectiveness_cols:\n",
    "                print(\"Cross-Modal Transfer Effectiveness (% reduction in attack success rate):\")\n",
    "                display(pivot[effectiveness_cols].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation Complexity Analysis\n",
    "\n",
    "Let's analyze the relationship between implementation complexity and security effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Define complexity scores (estimated based on boundary implementation)\n",
    "    complexity_scores = {\n",
    "        'none': 0,  # No implementation required\n",
    "        'token': 2,  # Moderate complexity\n",
    "        'semantic': 3,  # Higher complexity\n",
    "        'hybrid': 4   # Highest complexity\n",
    "    }\n",
    "    \n",
    "    # Calculate average prompt length by boundary type (a proxy for implementation complexity)\n",
    "    if 'prompt_length' in results_df.columns:\n",
    "        complexity_data = results_df.groupby('boundary')['prompt_length'].mean().reset_index()\n",
    "        complexity_data['complexity_score'] = complexity_data['boundary'].map(complexity_scores)\n",
    "        \n",
    "        # Calculate average success rate by boundary\n",
    "        success_data = results_df.groupby('boundary')['attack_success'].mean() * 100\n",
    "        complexity_data['attack_success_rate'] = complexity_data['boundary'].map(success_data)\n",
    "        \n",
    "        # Plot complexity vs effectiveness\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = plt.scatter(\n",
    "            complexity_data['prompt_length'],\n",
    "            100 - complexity_data['attack_success_rate'],  # Convert to protection rate (higher is better)\n",
    "            s=complexity_data['complexity_score'] * 50,  # Size based on complexity score\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, row in complexity_data.iterrows():\n",
    "            plt.annotate(\n",
    "                row['boundary'],\n",
    "                (row['prompt_length'], 100 - row['attack_success_rate']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=12\n",
    "            )\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Security Effectiveness vs. Implementation Complexity', fontsize=16)\n",
    "        plt.xlabel('Average Prompt Length (characters)', fontsize=14)\n",
    "        plt.ylabel('Protection Rate (%)', fontsize=14)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Add note about the plot\n",
    "        plt.figtext(\n",
    "            0.5, 0.01,\n",
    "            \"Higher protection rate and lower prompt length is better. Bubble size represents implementation complexity score.\",\n",
    "            ha='center',\n",
    "            fontsize=12\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display the data table\n",
    "        display_df = complexity_data.copy()\n",
    "        display_df['protection_rate'] = 100 - display_df['attack_success_rate']\n",
    "        display_df = display_df[['boundary', 'prompt_length', 'complexity_score', 'protection_rate']]\n",
    "        display_df.columns = ['Boundary Type', 'Avg Prompt Length', 'Complexity Score', 'Protection Rate (%)']\n",
    "        display(display_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Pattern Analysis\n",
    "\n",
    "Let's analyze which types of multi-modal attacks succeed against different boundary types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if results_df is not None:\n",
    "    # Create a heatmap of attack success rates by attack subtype and boundary\n",
    "    attack_pattern_data = pd.pivot_table(\n",
    "        results_df,\n",
    "        values='attack_success',\n",
    "        index=['attack_type', 'attack_subtype'],\n",
    "        columns=['boundary'],\n",
    "        aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "    )\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap
