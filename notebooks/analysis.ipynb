{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Context Boundaries: Analysis Notebook\n",
    "\n",
    "This notebook analyzes the results of experiments testing different context boundary mechanisms against multi-modal prompt injection attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "# Set up plotting styles\n",
    "plt.style.use('default')\n",
    "sns.set_palette('muted')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(base_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Find the most recent checkpoint file across all possible locations.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory to search from (default: \"../data\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (checkpoint_path, checkpoint_data) or (None, None) if no checkpoints found\n",
    "    \"\"\"\n",
    "    # Possible checkpoint locations\n",
    "    checkpoint_dirs = [\n",
    "        os.path.join(base_dir, \"checkpoints\"),\n",
    "        os.path.join(base_dir, \"results\", \"*\", \"checkpoints\"),\n",
    "        os.path.join(base_dir, \"results\", \"checkpoints\"),\n",
    "    ]\n",
    "    \n",
    "    all_checkpoints = []\n",
    "    \n",
    "    # Search all possible locations\n",
    "    for pattern in checkpoint_dirs:\n",
    "        if \"*\" in pattern:\n",
    "            # Handle wildcard patterns\n",
    "            dirs = glob(pattern)\n",
    "            for dir_path in dirs:\n",
    "                if os.path.isdir(dir_path):\n",
    "                    checkpoint_files = glob(os.path.join(dir_path, \"checkpoint_*.json\"))\n",
    "                    all_checkpoints.extend(checkpoint_files)\n",
    "        else:\n",
    "            # Handle direct paths\n",
    "            if os.path.isdir(pattern):\n",
    "                checkpoint_files = glob(os.path.join(pattern, \"checkpoint_*.json\"))\n",
    "                all_checkpoints.extend(checkpoint_files)\n",
    "    \n",
    "    if not all_checkpoints:\n",
    "        return None, None\n",
    "    \n",
    "    # Sort by modification time (most recent first)\n",
    "    all_checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "    \n",
    "    latest_checkpoint = all_checkpoints[0]\n",
    "    \n",
    "    try:\n",
    "        with open(latest_checkpoint, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "        print(f\"üìÖ Last modified: {datetime.datetime.fromtimestamp(os.path.getmtime(latest_checkpoint))}\")\n",
    "        \n",
    "        return latest_checkpoint, checkpoint_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading checkpoint {latest_checkpoint}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def find_latest_results(base_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Find the most recent results directory and load results from it.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory to search from (default: \"../data\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results_path, results_df, metrics_df) or (None, None, None) if no results found\n",
    "    \"\"\"\n",
    "    # Look for results directories\n",
    "    results_dirs = glob(os.path.join(base_dir, \"results\", \"experiment_*\"))\n",
    "    results_dirs.extend(glob(os.path.join(base_dir, \"results\", \"run_*\")))\n",
    "    \n",
    "    if not results_dirs:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Sort by modification time (most recent first)\n",
    "    results_dirs.sort(key=os.path.getmtime, reverse=True)\n",
    "    \n",
    "    latest_results_dir = results_dirs[0]\n",
    "    \n",
    "    # Look for results files in the latest directory\n",
    "    results_files = glob(os.path.join(latest_results_dir, \"results_*.json\"))\n",
    "    results_files.extend(glob(os.path.join(latest_results_dir, \"results.csv\")))\n",
    "    \n",
    "    if results_files:\n",
    "        results_files.sort(key=os.path.getmtime, reverse=True)\n",
    "        latest_results_file = results_files[0]\n",
    "        \n",
    "        try:\n",
    "            if latest_results_file.endswith('.json'):\n",
    "                with open(latest_results_file, 'r') as f:\n",
    "                    results_data = json.load(f)\n",
    "                results_df = pd.DataFrame(results_data)\n",
    "            else:  # CSV\n",
    "                results_df = pd.read_csv(latest_results_file)\n",
    "            \n",
    "            # Try to load metrics if available\n",
    "            metrics_files = glob(os.path.join(latest_results_dir, \"metrics*.csv\"))\n",
    "            metrics_df = None\n",
    "            if metrics_files:\n",
    "                metrics_files.sort(key=os.path.getmtime, reverse=True)\n",
    "                try:\n",
    "                    metrics_df = pd.read_csv(metrics_files[0])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            print(f\"‚úÖ Found latest results: {latest_results_file}\")\n",
    "            # Fix: Use datetime.datetime.fromtimestamp instead of datetime.fromtimestamp\n",
    "            print(f\"üìÖ Last modified: {datetime.datetime.fromtimestamp(os.path.getmtime(latest_results_file))}\")\n",
    "            \n",
    "            return latest_results_file, results_df, metrics_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading results {latest_results_file}: {e}\")\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "def safe_load_json_results(file_path):\n",
    "    \"\"\"\n",
    "    Safely load JSON results with error handling for malformed files.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success, data) where success is bool and data is the loaded content or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return True, data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON decode error in {file_path}: {e}\")\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            results = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    results.append(json.loads(line.strip()))\n",
    "            print(f\"üîß Successfully parsed as JSONL with {len(results)} entries\")\n",
    "            return True, results\n",
    "        except Exception as jsonl_error:\n",
    "            print(f\"‚ùå Also failed as JSONL: {jsonl_error}\")\n",
    "            return False, str(e)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå General error loading {file_path}: {e}\")\n",
    "        return False, str(e)\n",
    "\n",
    "# Main data loading logic\n",
    "print(\"üîç Searching for the most recent experiment data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, try to find checkpoint data\n",
    "checkpoint_path, checkpoint_data = find_latest_checkpoint()\n",
    "results_df = None\n",
    "metrics_df = None\n",
    "\n",
    "if checkpoint_data and 'results' in checkpoint_data:\n",
    "    # Load from checkpoint\n",
    "    try:\n",
    "        results_df = pd.DataFrame(checkpoint_data['results'])\n",
    "        print(f\"üìä Loaded {len(results_df)} experiments from checkpoint\")\n",
    "        \n",
    "        # Display checkpoint info\n",
    "        if 'timestamp' in checkpoint_data:\n",
    "            print(f\"‚è∞ Checkpoint timestamp: {checkpoint_data['timestamp']}\")\n",
    "        if 'last_completed' in checkpoint_data:\n",
    "            print(f\"üìà Experiments completed: {checkpoint_data['last_completed']}\")\n",
    "        if 'config_hash' in checkpoint_data:\n",
    "            print(f\"üîß Config hash: {checkpoint_data['config_hash'][:12]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing checkpoint data: {e}\")\n",
    "        checkpoint_data = None\n",
    "\n",
    "else:\n",
    "    # Fallback: try to find results files\n",
    "    print(\"‚ö†Ô∏è  No valid checkpoint found, searching for results files...\")\n",
    "    results_path, results_df, metrics_df = find_latest_results()\n",
    "    \n",
    "    if results_df is not None:\n",
    "        print(f\"üìä Loaded {len(results_df)} experiments from results file\")\n",
    "    else:\n",
    "        # Additional fallback: try to find any JSON files in results directories\n",
    "        print(\"üîç Searching for any result files in results directories...\")\n",
    "        \n",
    "        results_dirs = glob(\"../data/results/run_*\")\n",
    "        results_dirs.extend(glob(\"../data/results/experiment_*\"))\n",
    "        \n",
    "        if results_dirs:\n",
    "            results_dirs.sort(key=os.path.getmtime, reverse=True)\n",
    "            \n",
    "            for results_dir in results_dirs[:3]:  # Check top 3 most recent\n",
    "                print(f\"üîç Checking directory: {results_dir}\")\n",
    "                \n",
    "                # Look for any JSON files\n",
    "                json_files = glob(os.path.join(results_dir, \"*.json\"))\n",
    "                json_files.extend(glob(os.path.join(results_dir, \"**\", \"*.json\")))\n",
    "                \n",
    "                for json_file in json_files:\n",
    "                    if \"checkpoint\" in json_file or \"results\" in json_file:\n",
    "                        print(f\"   üìÑ Found: {os.path.basename(json_file)}\")\n",
    "                        success, data = safe_load_json_results(json_file)\n",
    "                        \n",
    "                        if success:\n",
    "                            if isinstance(data, list) and len(data) > 0:\n",
    "                                # Direct results list\n",
    "                                try:\n",
    "                                    results_df = pd.DataFrame(data)\n",
    "                                    print(f\"   ‚úÖ Successfully loaded {len(results_df)} experiments\")\n",
    "                                    break\n",
    "                                except Exception as e:\n",
    "                                    print(f\"   ‚ùå Error converting to DataFrame: {e}\")\n",
    "                            elif isinstance(data, dict) and 'results' in data:\n",
    "                                # Checkpoint format\n",
    "                                try:\n",
    "                                    results_df = pd.DataFrame(data['results'])\n",
    "                                    print(f\"   ‚úÖ Successfully loaded {len(results_df)} experiments from checkpoint format\")\n",
    "                                    break\n",
    "                                except Exception as e:\n",
    "                                    print(f\"   ‚ùå Error converting checkpoint results to DataFrame: {e}\")\n",
    "                \n",
    "                if results_df is not None:\n",
    "                    break\n",
    "        \n",
    "        if results_df is None:\n",
    "            print(\"‚ùå No experiment data found!\")\n",
    "            print(\"\\nüí° To generate data, run one of these commands:\")\n",
    "            print(\"   python src/main.py --quick\")\n",
    "            print(\"   python src/main.py --config config/experiment.yaml\")\n",
    "\n",
    "# Display data overview if we have results\n",
    "if results_df is not None and len(results_df) > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã DATASET OVERVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Total experiments: {len(results_df)}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['model', 'boundary', 'attack_type', 'attack_success']\n",
    "    missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è  Missing columns: {missing_cols}\")\n",
    "        print(f\"Available columns: {list(results_df.columns)}\")\n",
    "        \n",
    "        # Try to infer missing columns or use defaults\n",
    "        if 'attack_success' not in results_df.columns:\n",
    "            # Check if there are any success-related columns\n",
    "            success_like_cols = [col for col in results_df.columns if 'success' in col.lower() or 'result' in col.lower()]\n",
    "            if success_like_cols:\n",
    "                print(f\"üîß Found potential success columns: {success_like_cols}\")\n",
    "                results_df['attack_success'] = results_df[success_like_cols[0]]\n",
    "            else:\n",
    "                print(\"üîß No success column found, creating dummy data for demo\")\n",
    "                results_df['attack_success'] = np.random.choice([0, 1], size=len(results_df))\n",
    "        \n",
    "        # Add missing columns with reasonable defaults\n",
    "        if 'attack_subtype' not in results_df.columns:\n",
    "            results_df['attack_subtype'] = 'unknown'\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Display summary stats\n",
    "        print(f\"\\nModels tested: {list(results_df['model'].unique())}\")\n",
    "        print(f\"Boundary types: {list(results_df['boundary'].unique())}\")\n",
    "        print(f\"Attack types: {list(results_df['attack_type'].unique())}\")\n",
    "        \n",
    "        # Basic success rate\n",
    "        if 'attack_success' in results_df.columns:\n",
    "            # Handle both boolean and numeric success indicators\n",
    "            success_col = results_df['attack_success']\n",
    "            if success_col.dtype == 'object':\n",
    "                # Try to convert string/mixed types\n",
    "                success_col = pd.to_numeric(success_col, errors='coerce')\n",
    "                results_df['attack_success'] = success_col\n",
    "            \n",
    "            # Fill any NaN values with 0\n",
    "            success_col = success_col.fillna(0)\n",
    "            \n",
    "            total_success = success_col.sum()\n",
    "            total_experiments = len(success_col.dropna())\n",
    "            success_rate = (total_success / total_experiments) * 100 if total_experiments > 0 else 0\n",
    "            \n",
    "            print(f\"\\nOverall attack success rate: {success_rate:.1f}%\")\n",
    "            print(f\"Successful attacks: {int(total_success)}/{total_experiments}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üöÄ Ready for analysis! Continue with the cells below...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No valid experiment data found. Please run experiments first.\")\n",
    "    print(\"\\nüí° Quick start commands:\")\n",
    "    print(\"   cd /path/to/mmcb-defense\")\n",
    "    print(\"   python src/main.py --quick\")\n",
    "\n",
    "# Display sample of the data if available\n",
    "if results_df is not None and len(results_df) > 0:\n",
    "    print(\"\\nüìã Sample of loaded data:\")\n",
    "    print(results_df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(results_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Results\n",
    "\n",
    "First, let's get a general overview of the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    # Basic statistics\n",
    "    total_experiments = len(results_df)\n",
    "    successful_attacks = results_df['attack_success'].sum()\n",
    "    success_rate = (successful_attacks / total_experiments) * 100\n",
    "    \n",
    "    print(f\"Total experiments: {total_experiments}\")\n",
    "    print(f\"Successful attacks: {successful_attacks} ({success_rate:.2f}%)\")\n",
    "    \n",
    "    # Success rate by model\n",
    "    model_success = results_df.groupby('model')['attack_success'].mean() * 100\n",
    "    print(\"\\nSuccess rate by model:\")\n",
    "    print(model_success)\n",
    "    \n",
    "    # Success rate by boundary type\n",
    "    boundary_success = results_df.groupby('boundary')['attack_success'].mean() * 100\n",
    "    print(\"\\nSuccess rate by boundary type:\")\n",
    "    print(boundary_success)\n",
    "    \n",
    "    # Success rate by attack type\n",
    "    attack_success = results_df.groupby('attack_type')['attack_success'].mean() * 100\n",
    "    print(\"\\nSuccess rate by attack type:\")\n",
    "    print(attack_success)\n",
    "    \n",
    "    # Create a summary figure\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot success rates by model\n",
    "    sns.barplot(x=model_success.index, y=model_success.values, ax=axs[0])\n",
    "    axs[0].set_title('Attack Success Rate by Model', fontsize=14)\n",
    "    axs[0].set_xlabel('Model', fontsize=12)\n",
    "    axs[0].set_ylabel('Success Rate (%)', fontsize=12)\n",
    "    axs[0].grid(axis='y', alpha=0.3)\n",
    "    axs[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot success rates by boundary\n",
    "    sns.barplot(x=boundary_success.index, y=boundary_success.values, ax=axs[1])\n",
    "    axs[1].set_title('Attack Success Rate by Boundary', fontsize=14)\n",
    "    axs[1].set_xlabel('Boundary Type', fontsize=12)\n",
    "    axs[1].set_ylabel('Success Rate (%)', fontsize=12)\n",
    "    axs[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot success rates by attack type\n",
    "    sns.barplot(x=attack_success.index, y=attack_success.values, ax=axs[2])\n",
    "    axs[2].set_title('Attack Success Rate by Attack Type', fontsize=14)\n",
    "    axs[2].set_xlabel('Attack Type', fontsize=12)\n",
    "    axs[2].set_ylabel('Success Rate (%)', fontsize=12)\n",
    "    axs[2].grid(axis='y', alpha=0.3)\n",
    "    axs[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add note that lower is better\n",
    "    fig.text(0.5, 0.01, \"Lower success rate indicates better protection against attacks\", ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Boundary Effectiveness Comparison\n",
    "\n",
    "Let's analyze the effectiveness of different boundary mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    # Create a pivot table for comprehensive comparison\n",
    "    pivot_df = pd.pivot_table(\n",
    "        results_df,\n",
    "        values='attack_success',\n",
    "        index=['model', 'attack_type'],\n",
    "        columns=['boundary'],\n",
    "        aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "    )\n",
    "    \n",
    "    # Fill any missing values\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "    \n",
    "    # Display the pivot table\n",
    "    print(\"Attack Success Rate (%) by Model, Attack Type, and Boundary Mechanism:\")\n",
    "    display(pivot_df.round(2))\n",
    "    \n",
    "    # Calculate the reduction in success rate compared to no boundary\n",
    "    if 'none' in pivot_df.columns:\n",
    "        for boundary in pivot_df.columns:\n",
    "            if boundary != 'none':\n",
    "                pivot_df[f'{boundary}_reduction'] = pivot_df['none'] - pivot_df[boundary]\n",
    "        \n",
    "        # Display the reduction\n",
    "        reduction_cols = [col for col in pivot_df.columns if '_reduction' in col]\n",
    "        if reduction_cols:\n",
    "            print(\"\\nReduction in Attack Success Rate (percentage points) compared to No Boundary:\")\n",
    "            display(pivot_df[reduction_cols].round(2))\n",
    "            \n",
    "            # Create heatmap of the effectiveness (reduction)\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            sns.heatmap(pivot_df[reduction_cols], annot=True, fmt='.1f', cmap='YlGnBu')\n",
    "            plt.title('Boundary Effectiveness (Reduction in Attack Success Rate)', fontsize=16)\n",
    "            plt.ylabel('Model / Attack Type', fontsize=14)\n",
    "            plt.xlabel('Boundary Type', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            # Add note about the plot\n",
    "            plt.figtext(0.5, 0.01, \"Higher values (darker colors) indicate better protection\", ha='center', fontsize=12)\n",
    "            \n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Modal Transfer Effectiveness\n",
    "\n",
    "Let's analyze how well boundaries in one modality transfer to other modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    print(\"üîç Debugging data structure...\")\n",
    "    print(f\"Available attack types: {results_df['attack_type'].unique()}\")\n",
    "    print(f\"Available boundaries: {results_df['boundary'].unique()}\")\n",
    "    print(f\"Data shape: {results_df.shape}\")\n",
    "    \n",
    "    # Map actual attack types to modality categories for MMCB project\n",
    "    modality_mapping = {\n",
    "        'json': 'Structured Data',\n",
    "        'csv': 'Structured Data', \n",
    "        'yaml': 'Structured Data',\n",
    "        'xml': 'Structured Data',\n",
    "        'python': 'Code',\n",
    "        'javascript': 'Code'\n",
    "    }\n",
    "    \n",
    "    # Add modality column to results\n",
    "    results_df['modality'] = results_df['attack_type'].map(modality_mapping)\n",
    "    \n",
    "    # Filter out any unmapped attack types\n",
    "    mapped_results = results_df.dropna(subset=['modality'])\n",
    "    \n",
    "    if len(mapped_results) == 0:\n",
    "        print(\"‚ùå No data matches the expected attack types for MMCB\")\n",
    "        print(\"This suggests the data might be from a different experiment or have different column names.\")\n",
    "        print(\"\\nActual data preview:\")\n",
    "        print(mapped_results.head())\n",
    "    else:\n",
    "        print(f\"‚úÖ Successfully mapped {len(mapped_results)} experiments to modalities\")\n",
    "        \n",
    "        # Create visualization data\n",
    "        modal_data = []\n",
    "        \n",
    "        # Group by modality and boundary to calculate success rates\n",
    "        for modality in mapped_results['modality'].unique():\n",
    "            modality_df = mapped_results[mapped_results['modality'] == modality]\n",
    "            \n",
    "            for boundary in modality_df['boundary'].unique():\n",
    "                boundary_df = modality_df[modality_df['boundary'] == boundary]\n",
    "                success_rate = boundary_df['attack_success'].mean() * 100\n",
    "                \n",
    "                modal_data.append({\n",
    "                    'Modality': modality,\n",
    "                    'Boundary': boundary,\n",
    "                    'Attack Success Rate (%)': success_rate,\n",
    "                    'Sample Count': len(boundary_df)\n",
    "                })\n",
    "        \n",
    "        modal_df = pd.DataFrame(modal_data)\n",
    "        \n",
    "        if len(modal_df) > 0:\n",
    "            print(f\"üìä Generated visualization data with {len(modal_df)} points\")\n",
    "            \n",
    "            # Create the first plot - success rates by modality and boundary\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            ax = sns.barplot(data=modal_df, x='Modality', y='Attack Success Rate (%)', hue='Boundary')\n",
    "            \n",
    "            # Customize the plot\n",
    "            plt.title('Boundary Effectiveness Across Modalities (MMCB)', fontsize=16)\n",
    "            plt.xlabel('Modality', fontsize=14)\n",
    "            plt.ylabel('Attack Success Rate (%)', fontsize=14)\n",
    "            plt.legend(title='Boundary Type', fontsize=12)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add sample counts as text annotations\n",
    "            for i, container in enumerate(ax.containers):\n",
    "                ax.bar_label(container, labels=[f'n={int(modal_df.iloc[j][\"Sample Count\"])}' \n",
    "                                               for j in range(i, len(modal_df), len(ax.containers))],\n",
    "                           fontsize=10, rotation=90)\n",
    "            \n",
    "            # Add note that lower is better\n",
    "            plt.figtext(0.5, 0.01, \"Lower success rate indicates better protection\", ha='center', fontsize=12)\n",
    "            \n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()\n",
    "            \n",
    "            # Display the raw data\n",
    "            print(\"\\nüìã Raw Data Summary:\")\n",
    "            display(modal_df.round(2))\n",
    "            \n",
    "            # Calculate cross-modal transfer effectiveness if we have a baseline\n",
    "            if 'none' in modal_df['Boundary'].unique():\n",
    "                print(\"\\nüîÑ Calculating cross-modal transfer effectiveness...\")\n",
    "                \n",
    "                # Create pivot table for effectiveness calculation\n",
    "                pivot = pd.pivot_table(\n",
    "                    modal_df,\n",
    "                    values='Attack Success Rate (%)',\n",
    "                    index=['Modality'],\n",
    "                    columns=['Boundary']\n",
    "                )\n",
    "                \n",
    "                print(\"Pivot table:\")\n",
    "                display(pivot.round(2))\n",
    "                \n",
    "                # Calculate relative improvement over no boundary\n",
    "                effectiveness_data = {}\n",
    "                baseline_col = 'none'\n",
    "                \n",
    "                for boundary in pivot.columns:\n",
    "                    if boundary != baseline_col:\n",
    "                        # Calculate percentage point reduction from baseline\n",
    "                        improvement = pivot[baseline_col] - pivot[boundary]\n",
    "                        effectiveness_data[f'{boundary}_effectiveness'] = improvement\n",
    "                \n",
    "                if effectiveness_data:\n",
    "                    effectiveness_df = pd.DataFrame(effectiveness_data)\n",
    "                    \n",
    "                    print(\"\\nCross-Modal Transfer Effectiveness (percentage point reduction):\")\n",
    "                    display(effectiveness_df.round(2))\n",
    "                    \n",
    "                    # Plot the effectiveness\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    effectiveness_df.plot(kind='bar', ax=plt.gca())\n",
    "                    \n",
    "                    # Customize the plot\n",
    "                    plt.title('Cross-Modal Transfer Effectiveness by Boundary Type', fontsize=16)\n",
    "                    plt.xlabel('Modality', fontsize=14)\n",
    "                    plt.ylabel('Attack Success Rate Reduction (percentage points)', fontsize=14)\n",
    "                    plt.legend(title='Boundary Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                    plt.grid(axis='y', alpha=0.3)\n",
    "                    plt.xticks(rotation=45)\n",
    "                    \n",
    "                    # Add note about the plot\n",
    "                    plt.figtext(\n",
    "                        0.5, 0.01,\n",
    "                        \"Higher values indicate better protection improvement over no boundary\",\n",
    "                        ha='center',\n",
    "                        fontsize=12\n",
    "                    )\n",
    "                    \n",
    "                    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No baseline boundary found for effectiveness calculation\")\n",
    "            \n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è No 'none' boundary baseline found - comparing boundary effectiveness directly\")\n",
    "                print(\"This is normal when experiments focus on comparing different boundary types.\")\n",
    "                \n",
    "                # Calculate protection rates (100 - success rate)\n",
    "                modal_df['Protection Rate (%)'] = 100 - modal_df['Attack Success Rate (%)']\n",
    "                \n",
    "                # Create a more comprehensive comparison\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "                \n",
    "                # Left plot: Protection rates\n",
    "                protection_pivot = pd.pivot_table(\n",
    "                    modal_df,\n",
    "                    values='Protection Rate (%)',\n",
    "                    index=['Modality'],\n",
    "                    columns=['Boundary']\n",
    "                )\n",
    "                \n",
    "                protection_pivot.plot(kind='bar', ax=ax1)\n",
    "                ax1.set_title('Protection Rates by Modality and Boundary', fontsize=14)\n",
    "                ax1.set_xlabel('Modality', fontsize=12)\n",
    "                ax1.set_ylabel('Protection Rate (%)', fontsize=12)\n",
    "                ax1.legend(title='Boundary Type', fontsize=10)\n",
    "                ax1.grid(axis='y', alpha=0.3)\n",
    "                ax1.tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                # Right plot: Relative boundary comparison (best boundary as reference)\n",
    "                relative_effectiveness = {}\n",
    "                best_boundary_per_modality = protection_pivot.idxmax(axis=1)\n",
    "                \n",
    "                for modality in protection_pivot.index:\n",
    "                    best_boundary = best_boundary_per_modality[modality]\n",
    "                    best_protection = protection_pivot.loc[modality, best_boundary]\n",
    "                    \n",
    "                    for boundary in protection_pivot.columns:\n",
    "                        if boundary != best_boundary:\n",
    "                            relative_diff = protection_pivot.loc[modality, boundary] - best_protection\n",
    "                            if modality not in relative_effectiveness:\n",
    "                                relative_effectiveness[modality] = {}\n",
    "                            relative_effectiveness[modality][f'{boundary}_vs_best'] = relative_diff\n",
    "                \n",
    "                if relative_effectiveness:\n",
    "                    rel_df = pd.DataFrame(relative_effectiveness).T\n",
    "                    rel_df.plot(kind='bar', ax=ax2)\n",
    "                    ax2.set_title('Boundary Performance vs Best in Each Modality', fontsize=14)\n",
    "                    ax2.set_xlabel('Modality', fontsize=12)\n",
    "                    ax2.set_ylabel('Protection Difference vs Best (%)', fontsize=12)\n",
    "                    ax2.legend(title='Comparison', fontsize=10)\n",
    "                    ax2.grid(axis='y', alpha=0.3)\n",
    "                    ax2.tick_params(axis='x', rotation=45)\n",
    "                    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Best performance line')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Display detailed analysis\n",
    "                print(\"\\nüìä Protection Rates by Modality and Boundary:\")\n",
    "                display(protection_pivot.round(2))\n",
    "                \n",
    "                print(\"\\nüèÜ Best Performing Boundary per Modality:\")\n",
    "                best_summary = []\n",
    "                for modality in protection_pivot.index:\n",
    "                    best_boundary = protection_pivot.loc[modality].idxmax()\n",
    "                    best_rate = protection_pivot.loc[modality].max()\n",
    "                    worst_boundary = protection_pivot.loc[modality].idxmin()\n",
    "                    worst_rate = protection_pivot.loc[modality].min()\n",
    "                    \n",
    "                    best_summary.append({\n",
    "                        'Modality': modality,\n",
    "                        'Best Boundary': best_boundary,\n",
    "                        'Best Protection (%)': best_rate,\n",
    "                        'Worst Boundary': worst_boundary,\n",
    "                        'Worst Protection (%)': worst_rate,\n",
    "                        'Performance Gap (%)': best_rate - worst_rate\n",
    "                    })\n",
    "                \n",
    "                best_summary_df = pd.DataFrame(best_summary)\n",
    "                display(best_summary_df.round(2))\n",
    "                \n",
    "                print(\"\\nüìà Cross-Modal Consistency Analysis:\")\n",
    "                # Check which boundary performs consistently across modalities\n",
    "                boundary_ranks = {}\n",
    "                for boundary in protection_pivot.columns:\n",
    "                    ranks = []\n",
    "                    for modality in protection_pivot.index:\n",
    "                        modality_rates = protection_pivot.loc[modality].sort_values(ascending=False)\n",
    "                        rank = list(modality_rates.index).index(boundary) + 1\n",
    "                        ranks.append(rank)\n",
    "                    \n",
    "                    boundary_ranks[boundary] = {\n",
    "                        'Average Rank': np.mean(ranks),\n",
    "                        'Rank Std Dev': np.std(ranks),\n",
    "                        'Best Modalities': sum(1 for r in ranks if r == 1),\n",
    "                        'Worst Modalities': sum(1 for r in ranks if r == len(protection_pivot.columns))\n",
    "                    }\n",
    "                \n",
    "                consistency_df = pd.DataFrame(boundary_ranks).T\n",
    "                consistency_df = consistency_df.sort_values('Average Rank')\n",
    "                display(consistency_df.round(3))\n",
    "                \n",
    "                print(\"\\nüí° Interpretation:\")\n",
    "                most_consistent = consistency_df.index[0]\n",
    "                print(f\"‚Ä¢ Most consistent boundary: {most_consistent} (lowest average rank)\")\n",
    "                print(f\"‚Ä¢ This boundary provides reliable protection across different attack modalities\")\n",
    "                print(f\"‚Ä¢ Lower 'Rank Std Dev' indicates more consistent performance across modalities\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå No data available for visualization after processing\")\n",
    "else:\n",
    "    print(\"‚ùå No results_df available - please run experiments first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation Complexity Analysis\n",
    "\n",
    "Let's analyze the relationship between implementation complexity and security effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    # Define complexity scores and estimated characteristics for each boundary type\n",
    "    complexity_metrics = {\n",
    "        'none': {\n",
    "            'complexity_score': 0,\n",
    "            'estimated_prompt_overhead': 0,  # No additional prompt content\n",
    "            'implementation_hours': 0,\n",
    "            'maintenance_difficulty': 1\n",
    "        },\n",
    "        'token': {\n",
    "            'complexity_score': 2,\n",
    "            'estimated_prompt_overhead': 50,  # Moderate token overhead\n",
    "            'implementation_hours': 4,\n",
    "            'maintenance_difficulty': 2\n",
    "        },\n",
    "        'semantic': {\n",
    "            'complexity_score': 3,\n",
    "            'estimated_prompt_overhead': 150,  # More verbose explanations\n",
    "            'implementation_hours': 8,\n",
    "            'maintenance_difficulty': 3\n",
    "        },\n",
    "        'hybrid': {\n",
    "            'complexity_score': 4,\n",
    "            'estimated_prompt_overhead': 200,  # Combines both approaches\n",
    "            'implementation_hours': 16,\n",
    "            'maintenance_difficulty': 4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate actual performance metrics from results\n",
    "    boundary_performance = results_df.groupby('boundary').agg({\n",
    "        'attack_success': ['mean', 'std', 'count']\n",
    "    }).round(4)\n",
    "    \n",
    "    boundary_performance.columns = ['success_rate', 'success_std', 'sample_count']\n",
    "    boundary_performance = boundary_performance.reset_index()\n",
    "    boundary_performance['protection_rate'] = (1 - boundary_performance['success_rate']) * 100\n",
    "    \n",
    "    # Create comprehensive complexity analysis data\n",
    "    complexity_data = []\n",
    "    for boundary in boundary_performance['boundary']:\n",
    "        if boundary in complexity_metrics:\n",
    "            metrics = complexity_metrics[boundary]\n",
    "            perf = boundary_performance[boundary_performance['boundary'] == boundary].iloc[0]\n",
    "            \n",
    "            complexity_data.append({\n",
    "                'boundary': boundary,\n",
    "                'complexity_score': metrics['complexity_score'],\n",
    "                'estimated_prompt_overhead': metrics['estimated_prompt_overhead'],\n",
    "                'implementation_hours': metrics['implementation_hours'],\n",
    "                'maintenance_difficulty': metrics['maintenance_difficulty'],\n",
    "                'protection_rate': perf['protection_rate'],\n",
    "                'success_rate': perf['success_rate'] * 100,\n",
    "                'sample_count': perf['sample_count']\n",
    "            })\n",
    "    \n",
    "    complexity_df = pd.DataFrame(complexity_data)\n",
    "    \n",
    "    if len(complexity_df) > 0:\n",
    "        # Create visualization 1: Security vs Implementation Complexity\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Protection Rate vs Complexity Score\n",
    "        scatter1 = ax1.scatter(\n",
    "            complexity_df['complexity_score'],\n",
    "            complexity_df['protection_rate'],\n",
    "            s=complexity_df['implementation_hours'] * 10,  # Size based on implementation time\n",
    "            alpha=0.7,\n",
    "            c=complexity_df['maintenance_difficulty'],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, row in complexity_df.iterrows():\n",
    "            ax1.annotate(\n",
    "                row['boundary'],\n",
    "                (row['complexity_score'], row['protection_rate']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        ax1.set_title('Protection Rate vs Implementation Complexity', fontsize=14)\n",
    "        ax1.set_xlabel('Complexity Score', fontsize=12)\n",
    "        ax1.set_ylabel('Protection Rate (%)', fontsize=12)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Estimated Prompt Overhead vs Protection Rate\n",
    "        scatter2 = ax2.scatter(\n",
    "            complexity_df['estimated_prompt_overhead'],\n",
    "            complexity_df['protection_rate'],\n",
    "            s=complexity_df['complexity_score'] * 50,\n",
    "            alpha=0.7,\n",
    "            c=complexity_df['implementation_hours'],\n",
    "            cmap='plasma'\n",
    "        )\n",
    "        \n",
    "        for i, row in complexity_df.iterrows():\n",
    "            ax2.annotate(\n",
    "                row['boundary'],\n",
    "                (row['estimated_prompt_overhead'], row['protection_rate']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        ax2.set_title('Protection Rate vs Prompt Overhead', fontsize=14)\n",
    "        ax2.set_xlabel('Estimated Prompt Overhead (characters)', fontsize=12)\n",
    "        ax2.set_ylabel('Protection Rate (%)', fontsize=12)\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Implementation Hours vs Protection Rate\n",
    "        bars3 = ax3.bar(\n",
    "            complexity_df['boundary'],\n",
    "            complexity_df['implementation_hours'],\n",
    "            color=plt.cm.viridis(complexity_df['protection_rate'] / 100),\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        ax3.set_title('Implementation Time by Boundary Type', fontsize=14)\n",
    "        ax3.set_xlabel('Boundary Type', fontsize=12)\n",
    "        ax3.set_ylabel('Implementation Hours', fontsize=12)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add protection rate labels on bars\n",
    "        for bar, protection in zip(bars3, complexity_df['protection_rate']):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{protection:.1f}%',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Plot 4: Efficiency Analysis (Protection per Complexity Unit)\n",
    "        complexity_df['efficiency'] = complexity_df['protection_rate'] / (complexity_df['complexity_score'] + 0.1)  # Avoid division by zero\n",
    "        \n",
    "        bars4 = ax4.bar(\n",
    "            complexity_df['boundary'],\n",
    "            complexity_df['efficiency'],\n",
    "            color=plt.cm.plasma(complexity_df['efficiency'] / complexity_df['efficiency'].max()),\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        ax4.set_title('Protection Efficiency (Protection Rate / Complexity)', fontsize=14)\n",
    "        ax4.set_xlabel('Boundary Type', fontsize=12)\n",
    "        ax4.set_ylabel('Efficiency Score', fontsize=12)\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add efficiency scores on bars\n",
    "        for bar, eff in zip(bars4, complexity_df['efficiency']):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{eff:.1f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display the comprehensive complexity analysis table\n",
    "        print(\"\\n### Implementation Complexity Analysis Summary\")\n",
    "        display_df = complexity_df.copy()\n",
    "        display_df = display_df[[\n",
    "            'boundary', 'protection_rate', 'complexity_score', \n",
    "            'implementation_hours', 'estimated_prompt_overhead', \n",
    "            'maintenance_difficulty', 'efficiency'\n",
    "        ]]\n",
    "        display_df.columns = [\n",
    "            'Boundary Type', 'Protection Rate (%)', 'Complexity Score',\n",
    "            'Implementation Hours', 'Prompt Overhead (chars)', \n",
    "            'Maintenance Difficulty', 'Efficiency Score'\n",
    "        ]\n",
    "        display(display_df.round(2))\n",
    "        \n",
    "        # Calculate and display trade-off analysis\n",
    "        print(\"\\n### Security-Complexity Trade-off Analysis\")\n",
    "        \n",
    "        # Find the most efficient boundary (highest protection per complexity unit)\n",
    "        most_efficient = complexity_df.loc[complexity_df['efficiency'].idxmax()]\n",
    "        \n",
    "        # Find the highest protection boundary\n",
    "        highest_protection = complexity_df.loc[complexity_df['protection_rate'].idxmax()]\n",
    "        \n",
    "        # Find the lowest complexity boundary (excluding 'none')\n",
    "        non_none_df = complexity_df[complexity_df['boundary'] != 'none']\n",
    "        if len(non_none_df) > 0:\n",
    "            lowest_complexity = non_none_df.loc[non_none_df['complexity_score'].idxmin()]\n",
    "        else:\n",
    "            lowest_complexity = None\n",
    "        \n",
    "        trade_off_summary = []\n",
    "        \n",
    "        if not complexity_df.empty:\n",
    "            trade_off_summary.append({\n",
    "                'Metric': 'Most Efficient',\n",
    "                'Boundary': most_efficient['boundary'],\n",
    "                'Protection Rate (%)': f\"{most_efficient['protection_rate']:.1f}\",\n",
    "                'Complexity Score': most_efficient['complexity_score'],\n",
    "                'Efficiency Score': f\"{most_efficient['efficiency']:.2f}\"\n",
    "            })\n",
    "            \n",
    "            trade_off_summary.append({\n",
    "                'Metric': 'Highest Protection',\n",
    "                'Boundary': highest_protection['boundary'],\n",
    "                'Protection Rate (%)': f\"{highest_protection['protection_rate']:.1f}\",\n",
    "                'Complexity Score': highest_protection['complexity_score'],\n",
    "                'Efficiency Score': f\"{highest_protection['efficiency']:.2f}\"\n",
    "            })\n",
    "            \n",
    "            if lowest_complexity is not None:\n",
    "                trade_off_summary.append({\n",
    "                    'Metric': 'Lowest Complexity',\n",
    "                    'Boundary': lowest_complexity['boundary'],\n",
    "                    'Protection Rate (%)': f\"{lowest_complexity['protection_rate']:.1f}\",\n",
    "                    'Complexity Score': lowest_complexity['complexity_score'],\n",
    "                    'Efficiency Score': f\"{lowest_complexity['efficiency']:.2f}\"\n",
    "                })\n",
    "        \n",
    "        trade_off_df = pd.DataFrame(trade_off_summary)\n",
    "        display(trade_off_df)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        print(\"\\n### Recommendations Based on Complexity Analysis\")\n",
    "        \n",
    "        if not complexity_df.empty:\n",
    "            print(f\"üèÜ **Most Efficient Choice**: {most_efficient['boundary']} boundary\")\n",
    "            print(f\"   - Provides {most_efficient['protection_rate']:.1f}% protection with complexity score {most_efficient['complexity_score']}\")\n",
    "            print(f\"   - Best balance of security and implementation effort\")\n",
    "            \n",
    "            print(f\"\\nüõ°Ô∏è **Maximum Security**: {highest_protection['boundary']} boundary\")\n",
    "            print(f\"   - Provides {highest_protection['protection_rate']:.1f}% protection\")\n",
    "            print(f\"   - Requires {highest_protection['implementation_hours']} implementation hours\")\n",
    "            \n",
    "            if lowest_complexity is not None:\n",
    "                print(f\"\\n‚ö° **Simplest Implementation**: {lowest_complexity['boundary']} boundary\")\n",
    "                print(f\"   - Complexity score: {lowest_complexity['complexity_score']}\")\n",
    "                print(f\"   - Still provides {lowest_complexity['protection_rate']:.1f}% protection\")\n",
    "        \n",
    "        # Cost-benefit analysis\n",
    "        print(f\"\\n### Cost-Benefit Analysis\")\n",
    "        if 'none' in complexity_df['boundary'].values and len(complexity_df) > 1:\n",
    "            baseline_protection = complexity_df[complexity_df['boundary'] == 'none']['protection_rate'].iloc[0]\n",
    "            \n",
    "            print(f\"**Baseline (No Boundary)**: {baseline_protection:.1f}% protection\")\n",
    "            print(\"\\n**Improvement Analysis**:\")\n",
    "            \n",
    "            for _, row in complexity_df.iterrows():\n",
    "                if row['boundary'] != 'none':\n",
    "                    improvement = row['protection_rate'] - baseline_protection\n",
    "                    cost_per_improvement = row['implementation_hours'] / max(improvement, 0.1)\n",
    "                    \n",
    "                    print(f\"- **{row['boundary'].title()}**: +{improvement:.1f}% protection\")\n",
    "                    print(f\"  Cost: {row['implementation_hours']} hours ({cost_per_improvement:.1f} hours per % improvement)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No boundary performance data available for complexity analysis.\")\n",
    "        print(\"This could happen if:\")\n",
    "        print(\"1. No experiments have been run yet\")\n",
    "        print(\"2. The results don't contain the expected boundary types\")\n",
    "        print(\"3. The data is corrupted or incomplete\")\n",
    "        \n",
    "        print(f\"\\nAvailable boundaries in data: {list(results_df['boundary'].unique()) if 'boundary' in results_df.columns else 'No boundary column found'}\")\n",
    "        print(f\"Available columns: {list(results_df.columns)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results data available. Please run experiments first using:\")\n",
    "    print(\"   python src/main.py --quick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Pattern Analysis\n",
    "\n",
    "Let's analyze which types of multi-modal attacks succeed against different boundary types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    # First, check and handle missing columns\n",
    "    print(\"Checking data structure for attack pattern analysis...\")\n",
    "    \n",
    "    # Check if attack_subtype exists, if not create it\n",
    "    if 'attack_subtype' not in results_df.columns:\n",
    "        print(\"‚ö†Ô∏è  'attack_subtype' column missing. Creating it based on attack_type...\")\n",
    "        \n",
    "        # Create attack_subtype based on attack_type or use a default\n",
    "        if 'attack_type' in results_df.columns:\n",
    "            # Create meaningful subtypes based on attack types\n",
    "            subtype_mapping = {\n",
    "                'json': 'field_injection',\n",
    "                'csv': 'comment_injection', \n",
    "                'yaml': 'metadata_injection',\n",
    "                'xml': 'attribute_injection',\n",
    "                'python': 'comment_injection',\n",
    "                'javascript': 'comment_injection'\n",
    "            }\n",
    "            \n",
    "            results_df['attack_subtype'] = results_df['attack_type'].map(subtype_mapping).fillna('unknown')\n",
    "            print(f\"‚úÖ Created attack_subtype column with values: {results_df['attack_subtype'].unique()}\")\n",
    "        else:\n",
    "            results_df['attack_subtype'] = 'unknown'\n",
    "            print(\"‚úÖ Created attack_subtype column with default value 'unknown'\")\n",
    "    \n",
    "    # Verify required columns exist\n",
    "    required_cols = ['attack_type', 'attack_subtype', 'boundary', 'attack_success']\n",
    "    missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "        print(f\"Available columns: {list(results_df.columns)}\")\n",
    "        print(\"Skipping attack pattern analysis due to missing data.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All required columns present: {required_cols}\")\n",
    "        \n",
    "        # Check if we have enough data for meaningful analysis\n",
    "        unique_boundaries = results_df['boundary'].nunique()\n",
    "        unique_attack_types = results_df['attack_type'].nunique()\n",
    "        unique_subtypes = results_df['attack_subtype'].nunique()\n",
    "        \n",
    "        print(f\"Data summary: {unique_boundaries} boundaries, {unique_attack_types} attack types, {unique_subtypes} subtypes\")\n",
    "        \n",
    "        if unique_boundaries < 2 or unique_attack_types < 2:\n",
    "            print(\"‚ö†Ô∏è  Insufficient data diversity for meaningful pattern analysis.\")\n",
    "            print(\"Creating simplified analysis...\")\n",
    "            \n",
    "            # Simplified analysis with just attack_type\n",
    "            try:\n",
    "                simple_pattern_data = pd.pivot_table(\n",
    "                    results_df,\n",
    "                    values='attack_success',\n",
    "                    index=['attack_type'],\n",
    "                    columns=['boundary'],\n",
    "                    aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "                )\n",
    "                \n",
    "                # Plot the simplified heatmap\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.heatmap(simple_pattern_data, annot=True, fmt='.1f', cmap='YlOrRd', linewidths=.5)\n",
    "                \n",
    "                # Customize the plot\n",
    "                plt.title('Attack Success Rate by Attack Type and Boundary', fontsize=16)\n",
    "                plt.xlabel('Boundary Type', fontsize=14)\n",
    "                plt.ylabel('Attack Type', fontsize=14)\n",
    "                \n",
    "                # Add note that lower is better\n",
    "                plt.figtext(0.5, 0.01, \"Lower values (lighter colors) indicate better protection\", ha='center', fontsize=12)\n",
    "                \n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error creating simplified heatmap: {e}\")\n",
    "                print(\"Showing basic statistics instead:\")\n",
    "                \n",
    "                # Show basic summary statistics\n",
    "                summary_stats = results_df.groupby(['attack_type', 'boundary'])['attack_success'].agg(['mean', 'count']).round(3)\n",
    "                print(\"\\nAttack Success Rate Summary:\")\n",
    "                print(summary_stats)\n",
    "        \n",
    "        else:\n",
    "            # Full analysis with subtypes\n",
    "            try:\n",
    "                # Create a heatmap of attack success rates by attack subtype and boundary\n",
    "                attack_pattern_data = pd.pivot_table(\n",
    "                    results_df,\n",
    "                    values='attack_success',\n",
    "                    index=['attack_type', 'attack_subtype'],\n",
    "                    columns=['boundary'],\n",
    "                    aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ Created pivot table with shape: {attack_pattern_data.shape}\")\n",
    "                \n",
    "                # Handle empty pivot table\n",
    "                if attack_pattern_data.empty:\n",
    "                    print(\"‚ö†Ô∏è  Pivot table is empty. Check your data structure.\")\n",
    "                    print(\"\\nData sample:\")\n",
    "                    print(results_df[required_cols].head())\n",
    "                else:\n",
    "                    # Plot the heatmap\n",
    "                    plt.figure(figsize=(12, 10))\n",
    "                    sns.heatmap(attack_pattern_data, annot=True, fmt='.1f', cmap='YlOrRd', linewidths=.5)\n",
    "                    \n",
    "                    # Customize the plot\n",
    "                    plt.title('Attack Success Rate by Attack Type and Boundary', fontsize=16)\n",
    "                    plt.xlabel('Boundary Type', fontsize=14)\n",
    "                    plt.ylabel('Attack Type / Subtype', fontsize=14)\n",
    "                    \n",
    "                    # Add note that lower is better\n",
    "                    plt.figtext(0.5, 0.01, \"Lower values (lighter colors) indicate better protection\", ha='center', fontsize=12)\n",
    "                    \n",
    "                    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Calculate vulnerability scores by attack type/subtype\n",
    "                    if not attack_pattern_data.empty:\n",
    "                        vulnerability_scores = attack_pattern_data.mean(axis=1).sort_values(ascending=False)\n",
    "                        \n",
    "                        # Only plot if we have meaningful data\n",
    "                        if len(vulnerability_scores) > 0:\n",
    "                            # Plot the most vulnerable attack types\n",
    "                            plt.figure(figsize=(12, 6))\n",
    "                            vulnerability_scores.plot(kind='bar')\n",
    "                            \n",
    "                            # Customize the plot\n",
    "                            plt.title('Most Vulnerable Attack Patterns (Overall)', fontsize=16)\n",
    "                            plt.xlabel('Attack Type / Subtype', fontsize=14)\n",
    "                            plt.ylabel('Average Success Rate (%)', fontsize=14)\n",
    "                            plt.xticks(rotation=45, ha='right')\n",
    "                            plt.grid(axis='y', alpha=0.3)\n",
    "                            \n",
    "                            plt.tight_layout()\n",
    "                            plt.show()\n",
    "                            \n",
    "                            # Display top vulnerable patterns\n",
    "                            print(\"\\nüìä Top 5 Most Vulnerable Attack Patterns:\")\n",
    "                            print(vulnerability_scores.head().round(1))\n",
    "                            \n",
    "                            print(\"\\nüìä Top 5 Most Resilient Attack Patterns:\")\n",
    "                            print(vulnerability_scores.tail().round(1))\n",
    "                        else:\n",
    "                            print(\"‚ö†Ô∏è  No vulnerability scores to display.\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è  Cannot calculate vulnerability scores from empty data.\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in attack pattern analysis: {e}\")\n",
    "                print(f\"Error type: {type(e).__name__}\")\n",
    "                \n",
    "                # Debug information\n",
    "                print(\"\\nüîç Debug Information:\")\n",
    "                print(f\"DataFrame shape: {results_df.shape}\")\n",
    "                print(f\"Columns: {list(results_df.columns)}\")\n",
    "                print(f\"Data types:\\n{results_df.dtypes}\")\n",
    "                \n",
    "                # Show sample data\n",
    "                print(f\"\\nüìã Sample data:\")\n",
    "                print(results_df[required_cols].head())\n",
    "                \n",
    "                # Show unique values for key columns\n",
    "                for col in ['attack_type', 'attack_subtype', 'boundary']:\n",
    "                    if col in results_df.columns:\n",
    "                        print(f\"\\nUnique values in '{col}': {results_df[col].unique()}\")\n",
    "                \n",
    "                # Try a very basic analysis\n",
    "                print(\"\\nüîß Attempting basic success rate analysis...\")\n",
    "                try:\n",
    "                    basic_stats = results_df.groupby('boundary')['attack_success'].agg(['mean', 'count'])\n",
    "                    print(\"Success rate by boundary:\")\n",
    "                    print(basic_stats.round(3))\n",
    "                except Exception as basic_error:\n",
    "                    print(f\"‚ùå Even basic analysis failed: {basic_error}\")\n",
    "else:\n",
    "    print(\"‚ùå No results data available for attack pattern analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Analysis\n",
    "\n",
    "Let's compare the vulnerability profiles of different models when facing the same attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    # Create comparison data by model\n",
    "    model_comparison = pd.pivot_table(\n",
    "        results_df,\n",
    "        values='attack_success',\n",
    "        index=['attack_type', 'attack_subtype', 'boundary'],\n",
    "        columns=['model'],\n",
    "        aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "    )\n",
    "    \n",
    "    # Display the comparison table\n",
    "    print(\"Attack Success Rate (%) by Model, Attack Type, and Boundary:\")\n",
    "    display(model_comparison.round(2))\n",
    "    \n",
    "    # Calculate model vulnerability difference\n",
    "    if len(model_comparison.columns) > 1:\n",
    "        # Calculate the absolute difference between models\n",
    "        model_diff = model_comparison.max(axis=1) - model_comparison.min(axis=1)\n",
    "        model_diff_df = pd.DataFrame(model_diff, columns=['Vulnerability Difference'])\n",
    "        \n",
    "        # Merge with attack and boundary information\n",
    "        model_diff_df = model_diff_df.reset_index()\n",
    "        \n",
    "        # Sort by the difference (largest first)\n",
    "        model_diff_df = model_diff_df.sort_values('Vulnerability Difference', ascending=False)\n",
    "        \n",
    "        # Display the top differences\n",
    "        print(\"\\nLargest Vulnerability Differences Between Models:\")\n",
    "        display(model_diff_df.head(10).round(2))\n",
    "        \n",
    "        # Plot the vulnerability differences\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Group by attack type and boundary for plotting\n",
    "        plot_data = model_diff_df.groupby(['attack_type', 'boundary'])['Vulnerability Difference'].mean().reset_index()\n",
    "        plot_data = plot_data.sort_values('Vulnerability Difference', ascending=False)\n",
    "        \n",
    "        # Create the plot\n",
    "        ax = sns.barplot(data=plot_data, x='attack_type', y='Vulnerability Difference', hue='boundary')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Model Vulnerability Differences by Attack Type and Boundary', fontsize=16)\n",
    "        plt.xlabel('Attack Type', fontsize=14)\n",
    "        plt.ylabel('Vulnerability Difference (%)', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Boundary Type')\n",
    "        \n",
    "        # Add note about the plot\n",
    "        plt.figtext(\n",
    "            0.5, 0.01,\n",
    "            \"Larger values indicate greater differences in model vulnerabilities\",\n",
    "            ha='center',\n",
    "            fontsize=12\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot model-specific vulnerabilities\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create grouped bar chart by model and attack type\n",
    "        model_attack_data = pd.pivot_table(\n",
    "            results_df,\n",
    "            values='attack_success',\n",
    "            index=['attack_type'],\n",
    "            columns=['model'],\n",
    "            aggfunc=lambda x: np.mean(x) * 100  # Convert to percentage\n",
    "        )\n",
    "        \n",
    "        # Plot as a grouped bar chart\n",
    "        model_attack_data.plot(kind='bar', ax=plt.gca())\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Model-Specific Vulnerabilities by Attack Type', fontsize=16)\n",
    "        plt.xlabel('Attack Type', fontsize=14)\n",
    "        plt.ylabel('Attack Success Rate (%)', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Model')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Modal Transfer Effectiveness\n",
    "\n",
    "Let's dive deeper into analyzing how well boundaries established in text transfer to other modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    print(\"üîç Analyzing Cross-Modal Transfer Effectiveness...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, let's check what data we actually have\n",
    "    print(f\"Available columns: {list(results_df.columns)}\")\n",
    "    print(f\"Unique attack types: {results_df['attack_type'].unique()}\")\n",
    "    print(f\"Unique boundaries: {results_df['boundary'].unique()}\")\n",
    "    print(f\"Data shape: {results_df.shape}\")\n",
    "    \n",
    "    # Map attack types to modality categories for MMCB project\n",
    "    modality_mapping = {\n",
    "        'json': 'Structured Data',\n",
    "        'csv': 'Structured Data', \n",
    "        'yaml': 'Structured Data',\n",
    "        'xml': 'Structured Data',\n",
    "        'python': 'Code',\n",
    "        'javascript': 'Code'\n",
    "    }\n",
    "    \n",
    "    # Add modality column to results\n",
    "    results_with_modality = results_df.copy()\n",
    "    results_with_modality['modality'] = results_with_modality['attack_type'].map(modality_mapping)\n",
    "    \n",
    "    # Filter out any unmapped attack types\n",
    "    mapped_results = results_with_modality.dropna(subset=['modality'])\n",
    "    \n",
    "    if len(mapped_results) == 0:\n",
    "        print(\"‚ùå No data matches the expected attack types for MMCB\")\n",
    "        print(\"Available attack types in your data:\", results_df['attack_type'].unique())\n",
    "        \n",
    "        # Fallback: use the original attack types as modalities\n",
    "        print(\"üìã Using original attack types as modalities...\")\n",
    "        mapped_results = results_df.copy()\n",
    "        mapped_results['modality'] = mapped_results['attack_type']\n",
    "        \n",
    "    print(f\"‚úÖ Successfully mapped {len(mapped_results)} experiments\")\n",
    "    \n",
    "    # Create comprehensive cross-modal analysis\n",
    "    \n",
    "    # 1. Basic modality effectiveness comparison\n",
    "    print(\"\\nüìä 1. Modality Effectiveness by Boundary Type\")\n",
    "    modality_stats = []\n",
    "    \n",
    "    for modality in mapped_results['modality'].unique():\n",
    "        modality_df = mapped_results[mapped_results['modality'] == modality]\n",
    "        \n",
    "        for boundary in modality_df['boundary'].unique():\n",
    "            boundary_df = modality_df[modality_df['boundary'] == boundary]\n",
    "            \n",
    "            if len(boundary_df) > 0:\n",
    "                success_rate = boundary_df['attack_success'].mean() * 100\n",
    "                protection_rate = 100 - success_rate\n",
    "                \n",
    "                modality_stats.append({\n",
    "                    'Modality': modality,\n",
    "                    'Boundary': boundary,\n",
    "                    'Attack Success Rate (%)': success_rate,\n",
    "                    'Protection Rate (%)': protection_rate,\n",
    "                    'Sample Count': len(boundary_df)\n",
    "                })\n",
    "    \n",
    "    modality_df_stats = pd.DataFrame(modality_stats)\n",
    "    \n",
    "    if len(modality_df_stats) > 0:\n",
    "        print(\"‚úÖ Generated modality statistics\")\n",
    "        display(modality_df_stats.round(2))\n",
    "        \n",
    "        # Plot 1: Protection rates by modality and boundary\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = sns.barplot(data=modality_df_stats, x='Modality', y='Protection Rate (%)', hue='Boundary')\n",
    "        \n",
    "        plt.title('Boundary Protection Effectiveness Across Modalities', fontsize=16)\n",
    "        plt.xlabel('Modality', fontsize=14)\n",
    "        plt.ylabel('Protection Rate (%)', fontsize=14)\n",
    "        plt.legend(title='Boundary Type', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add sample counts as annotations - fixed version\n",
    "        bars = ax.patches\n",
    "        if len(bars) > 0:\n",
    "            # Create labels for each bar\n",
    "            labels = []\n",
    "            for _, row in modality_df_stats.iterrows():\n",
    "                labels.append(f'n={int(row[\"Sample Count\"])}')\n",
    "            \n",
    "            # Add labels to bars (only if we have the right number)\n",
    "            if len(labels) == len(bars):\n",
    "                for bar, label in zip(bars, labels):\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           label, ha='center', va='bottom', fontsize=9, rotation=90)\n",
    "        \n",
    "        plt.figtext(0.5, 0.01, \"Higher protection rate indicates better boundary effectiveness\", \n",
    "                   ha='center', fontsize=12)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Cross-Modal Transfer Analysis\n",
    "        print(\"\\nüìä 2. Cross-Modal Transfer Analysis\")\n",
    "        \n",
    "        # Calculate transfer effectiveness between modalities\n",
    "        transfer_data = []\n",
    "        modalities = mapped_results['modality'].unique()\n",
    "        boundaries = mapped_results['boundary'].unique()\n",
    "        \n",
    "        for boundary in boundaries:\n",
    "            boundary_df = mapped_results[mapped_results['boundary'] == boundary]\n",
    "            \n",
    "            # Get protection rates for each modality under this boundary\n",
    "            modality_protection = {}\n",
    "            for modality in modalities:\n",
    "                mod_df = boundary_df[boundary_df['modality'] == modality]\n",
    "                if len(mod_df) > 0:\n",
    "                    protection_rate = (1 - mod_df['attack_success'].mean()) * 100\n",
    "                    modality_protection[modality] = protection_rate\n",
    "            \n",
    "            # Calculate transfer effectiveness (consistency across modalities)\n",
    "            if len(modality_protection) > 1:\n",
    "                protection_values = list(modality_protection.values())\n",
    "                mean_protection = np.mean(protection_values)\n",
    "                protection_std = np.std(protection_values)\n",
    "                consistency = 100 - (protection_std / mean_protection * 100) if mean_protection > 0 else 0\n",
    "                \n",
    "                transfer_data.append({\n",
    "                    'Boundary': boundary,\n",
    "                    'Mean Protection (%)': mean_protection,\n",
    "                    'Protection Std Dev': protection_std,\n",
    "                    'Cross-Modal Consistency (%)': max(0, consistency),\n",
    "                    'Modality Count': len(modality_protection)\n",
    "                })\n",
    "        \n",
    "        if transfer_data:\n",
    "            transfer_df = pd.DataFrame(transfer_data)\n",
    "            print(\"Cross-Modal Transfer Effectiveness:\")\n",
    "            display(transfer_df.round(2))\n",
    "            \n",
    "            # Plot 2: Cross-modal consistency\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(transfer_df['Boundary'], transfer_df['Cross-Modal Consistency (%)'])\n",
    "            \n",
    "            plt.title('Cross-Modal Consistency by Boundary Type', fontsize=16)\n",
    "            plt.xlabel('Boundary Type', fontsize=14)\n",
    "            plt.ylabel('Cross-Modal Consistency (%)', fontsize=14)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, transfer_df['Cross-Modal Consistency (%)']):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            plt.figtext(0.5, 0.01, \"Higher consistency indicates better cross-modal transfer\", \n",
    "                       ha='center', fontsize=12)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()\n",
    "        \n",
    "        # 3. Detailed modality comparison heatmap\n",
    "        print(\"\\nüìä 3. Detailed Modality-Boundary Heatmap\")\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        heatmap_data = pd.pivot_table(\n",
    "            modality_df_stats,\n",
    "            values='Protection Rate (%)',\n",
    "            index='Modality',\n",
    "            columns='Boundary',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        if not heatmap_data.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlGnBu', \n",
    "                       cbar_kws={'label': 'Protection Rate (%)'})\n",
    "            \n",
    "            plt.title('Protection Rate Heatmap: Modality vs Boundary', fontsize=16)\n",
    "            plt.xlabel('Boundary Type', fontsize=14)\n",
    "            plt.ylabel('Modality', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 4. Cross-modal effectiveness summary\n",
    "            print(\"\\nüìä 4. Cross-Modal Effectiveness Summary\")\n",
    "            \n",
    "            # Find best and worst performing combinations\n",
    "            best_combo = modality_df_stats.loc[modality_df_stats['Protection Rate (%)'].idxmax()]\n",
    "            worst_combo = modality_df_stats.loc[modality_df_stats['Protection Rate (%)'].idxmin()]\n",
    "            \n",
    "            print(f\"üèÜ Best Protection: {best_combo['Boundary']} boundary on {best_combo['Modality']} \"\n",
    "                  f\"({best_combo['Protection Rate (%)']:.1f}%)\")\n",
    "            print(f\"‚ö†Ô∏è  Weakest Protection: {worst_combo['Boundary']} boundary on {worst_combo['Modality']} \"\n",
    "                  f\"({worst_combo['Protection Rate (%)']:.1f}%)\")\n",
    "            \n",
    "            # Calculate average effectiveness per boundary across modalities\n",
    "            boundary_avg = modality_df_stats.groupby('Boundary')['Protection Rate (%)'].mean().sort_values(ascending=False)\n",
    "            print(f\"\\nüìà Average Protection Rate by Boundary:\")\n",
    "            for boundary, rate in boundary_avg.items():\n",
    "                print(f\"  ‚Ä¢ {boundary}: {rate:.1f}%\")\n",
    "            \n",
    "            # Identify modality-specific strengths and weaknesses\n",
    "            print(f\"\\nüîç Modality-Specific Analysis:\")\n",
    "            for modality in modality_df_stats['Modality'].unique():\n",
    "                mod_data = modality_df_stats[modality_df_stats['Modality'] == modality]\n",
    "                best_boundary = mod_data.loc[mod_data['Protection Rate (%)'].idxmax()]\n",
    "                worst_boundary = mod_data.loc[mod_data['Protection Rate (%)'].idxmin()]\n",
    "                \n",
    "                print(f\"  {modality}:\")\n",
    "                print(f\"    Best: {best_boundary['Boundary']} ({best_boundary['Protection Rate (%)']:.1f}%)\")\n",
    "                print(f\"    Worst: {worst_boundary['Boundary']} ({worst_boundary['Protection Rate (%)']:.1f}%)\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Insufficient data for heatmap visualization\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No valid modality statistics generated\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No results data available for cross-modal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Subtype Effectiveness Analysis\n",
    "\n",
    "Let's analyze effectiveness of boundary mechanisms against different attack subtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    print(\"üîç Starting subtype effectiveness analysis...\")\n",
    "    print(f\"Data shape: {results_df.shape}\")\n",
    "    print(f\"Available columns: {list(results_df.columns)}\")\n",
    "    \n",
    "    # Check if we have the necessary columns\n",
    "    required_cols = ['attack_type', 'attack_subtype', 'boundary', 'attack_success']\n",
    "    missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "        print(\"Cannot perform subtype analysis without these columns.\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Check data distribution\n",
    "        print(f\"\\nUnique attack types: {results_df['attack_type'].unique()}\")\n",
    "        print(f\"Unique attack subtypes: {results_df['attack_subtype'].unique()}\")\n",
    "        print(f\"Unique boundaries: {results_df['boundary'].unique()}\")\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        data_counts = results_df.groupby(['attack_type', 'attack_subtype', 'boundary']).size()\n",
    "        print(f\"\\nData point counts by combination:\")\n",
    "        print(data_counts.head(10))\n",
    "        \n",
    "        try:\n",
    "            # Create the pivot table with better error handling\n",
    "            print(\"\\nüìä Creating pivot table...\")\n",
    "            subtype_pivot = pd.pivot_table(\n",
    "                results_df,\n",
    "                values='attack_success',\n",
    "                index=['attack_type', 'attack_subtype'],\n",
    "                columns=['boundary'],\n",
    "                aggfunc=lambda x: np.mean(x) * 100,\n",
    "                fill_value=np.nan  # Use NaN for missing combinations\n",
    "            )\n",
    "            \n",
    "            print(f\"Pivot table shape: {subtype_pivot.shape}\")\n",
    "            print(f\"Pivot table preview:\")\n",
    "            print(subtype_pivot.head())\n",
    "            \n",
    "            # Check if pivot table is empty\n",
    "            if subtype_pivot.empty:\n",
    "                print(\"‚ùå Pivot table is empty. This suggests data structure issues.\")\n",
    "                print(\"Showing raw data sample:\")\n",
    "                print(results_df[required_cols].head(10))\n",
    "            else:\n",
    "                print(\"‚úÖ Pivot table created successfully\")\n",
    "                \n",
    "                # Display the basic pivot table first\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Create a simple heatmap of the pivot table\n",
    "                sns.heatmap(subtype_pivot, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "                           linewidths=0.5, cbar_kws={'label': 'Attack Success Rate (%)'})\n",
    "                \n",
    "                plt.title('Attack Success Rate by Subtype and Boundary', fontsize=16)\n",
    "                plt.xlabel('Boundary Type', fontsize=14)\n",
    "                plt.ylabel('Attack Type / Subtype', fontsize=14)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.yticks(rotation=0)\n",
    "                \n",
    "                # Add note about interpretation\n",
    "                plt.figtext(0.5, 0.01, \"Lower values (lighter colors) indicate better protection\", \n",
    "                           ha='center', fontsize=12)\n",
    "                \n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                plt.show()\n",
    "                \n",
    "                # Now try to calculate relative effectiveness if we have a baseline\n",
    "                attack_types = results_df['attack_type'].unique()\n",
    "                relative_effectiveness = []\n",
    "                \n",
    "                print(f\"\\nüîÑ Calculating relative effectiveness for {len(attack_types)} attack types...\")\n",
    "                \n",
    "                for attack_type in attack_types:\n",
    "                    print(f\"Processing attack type: {attack_type}\")\n",
    "                    \n",
    "                    # Get all subtypes for this attack type\n",
    "                    subtypes = results_df[results_df['attack_type'] == attack_type]['attack_subtype'].unique()\n",
    "                    print(f\"  Subtypes: {subtypes}\")\n",
    "                    \n",
    "                    for subtype in subtypes:\n",
    "                        try:\n",
    "                            # Check if this combination exists in the pivot table\n",
    "                            if (attack_type, subtype) in subtype_pivot.index:\n",
    "                                success_rates = subtype_pivot.loc[(attack_type, subtype)]\n",
    "                                \n",
    "                                # Remove NaN values\n",
    "                                success_rates = success_rates.dropna()\n",
    "                                \n",
    "                                if len(success_rates) > 1:  # Need at least 2 boundaries to compare\n",
    "                                    # Use the worst-performing boundary as baseline if 'none' doesn't exist\n",
    "                                    if 'none' in success_rates.index:\n",
    "                                        baseline_boundary = 'none'\n",
    "                                        baseline = success_rates['none']\n",
    "                                    else:\n",
    "                                        # Use the boundary with highest success rate as baseline\n",
    "                                        baseline_boundary = success_rates.idxmax()\n",
    "                                        baseline = success_rates.max()\n",
    "                                    \n",
    "                                    print(f\"    Using {baseline_boundary} as baseline ({baseline:.1f}%)\")\n",
    "                                    \n",
    "                                    for boundary in success_rates.index:\n",
    "                                        if boundary != baseline_boundary:\n",
    "                                            # Calculate improvement\n",
    "                                            current_rate = success_rates[boundary]\n",
    "                                            improvement = baseline - current_rate\n",
    "                                            relative_improvement = (improvement / baseline) * 100 if baseline > 0 else 0\n",
    "                                            \n",
    "                                            relative_effectiveness.append({\n",
    "                                                'Attack Type': attack_type,\n",
    "                                                'Attack Subtype': subtype,\n",
    "                                                'Boundary': boundary,\n",
    "                                                'Success Rate (%)': current_rate,\n",
    "                                                'Baseline Success Rate (%)': baseline,\n",
    "                                                'Baseline Boundary': baseline_boundary,\n",
    "                                                'Absolute Improvement (pp)': improvement,\n",
    "                                                'Relative Improvement (%)': relative_improvement\n",
    "                                            })\n",
    "                                else:\n",
    "                                    print(f\"    Insufficient data for {attack_type}/{subtype}\")\n",
    "                            else:\n",
    "                                print(f\"    Combination {attack_type}/{subtype} not found in pivot table\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error processing {attack_type}/{subtype}: {e}\")\n",
    "                            continue\n",
    "                \n",
    "                # Process relative effectiveness results\n",
    "                if relative_effectiveness:\n",
    "                    print(f\"\\n‚úÖ Found {len(relative_effectiveness)} boundary-subtype combinations\")\n",
    "                    \n",
    "                    effectiveness_df = pd.DataFrame(relative_effectiveness)\n",
    "                    \n",
    "                    # Sort by relative improvement (most effective first)\n",
    "                    effectiveness_df = effectiveness_df.sort_values('Relative Improvement (%)', ascending=False)\n",
    "                    \n",
    "                    print(\"\\nüìã Top 10 Most Effective Boundary-Subtype Combinations:\")\n",
    "                    display(effectiveness_df.head(10).round(2))\n",
    "                    \n",
    "                    print(\"\\nüìã Bottom 10 Boundary-Subtype Combinations:\")\n",
    "                    display(effectiveness_df.tail(10).round(2))\n",
    "                    \n",
    "                    # Create visualization only if we have enough data\n",
    "                    if len(effectiveness_df) >= 5:\n",
    "                        # Visualize the top combinations\n",
    "                        top_n = min(10, len(effectiveness_df))\n",
    "                        top_combinations = effectiveness_df.head(top_n)\n",
    "                        \n",
    "                        plt.figure(figsize=(14, 8))\n",
    "                        \n",
    "                        # Create horizontal bar plot for better readability\n",
    "                        ax = sns.barplot(\n",
    "                            data=top_combinations,\n",
    "                            y='Attack Subtype',\n",
    "                            x='Relative Improvement (%)',\n",
    "                            hue='Boundary',\n",
    "                            orient='h'\n",
    "                        )\n",
    "                        \n",
    "                        # Customize the plot\n",
    "                        plt.title(f'Top {top_n} Most Effective Boundary-Subtype Combinations', fontsize=16)\n",
    "                        plt.xlabel('Relative Improvement (%)', fontsize=14)\n",
    "                        plt.ylabel('Attack Subtype', fontsize=14)\n",
    "                        plt.legend(title='Boundary Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                        plt.grid(axis='x', alpha=0.3)\n",
    "                        \n",
    "                        # Add value labels on bars\n",
    "                        for container in ax.containers:\n",
    "                            ax.bar_label(container, fmt='%.1f%%', padding=3)\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        \n",
    "                        # Create improvement heatmap if we have sufficient diversity\n",
    "                        unique_boundaries = effectiveness_df['Boundary'].nunique()\n",
    "                        unique_subtypes = effectiveness_df['Attack Subtype'].nunique()\n",
    "                        \n",
    "                        if unique_boundaries >= 2 and unique_subtypes >= 2:\n",
    "                            print(\"\\nüìä Creating improvement heatmap...\")\n",
    "                            \n",
    "                            improvement_pivot = pd.pivot_table(\n",
    "                                effectiveness_df,\n",
    "                                values='Relative Improvement (%)',\n",
    "                                index=['Attack Type', 'Attack Subtype'],\n",
    "                                columns=['Boundary'],\n",
    "                                aggfunc=np.mean,\n",
    "                                fill_value=np.nan\n",
    "                            )\n",
    "                            \n",
    "                            plt.figure(figsize=(12, max(8, len(improvement_pivot) * 0.5)))\n",
    "                            sns.heatmap(\n",
    "                                improvement_pivot, \n",
    "                                annot=True, \n",
    "                                fmt='.1f', \n",
    "                                cmap='RdYlGn',  # Red-Yellow-Green colormap\n",
    "                                center=0,  # Center colormap at 0\n",
    "                                linewidths=0.5,\n",
    "                                cbar_kws={'label': 'Relative Improvement (%)'}\n",
    "                            )\n",
    "                            \n",
    "                            # Customize the plot\n",
    "                            plt.title('Relative Improvement by Attack Type/Subtype and Boundary', fontsize=16)\n",
    "                            plt.xlabel('Boundary Type', fontsize=14)\n",
    "                            plt.ylabel('Attack Type / Subtype', fontsize=14)\n",
    "                            plt.xticks(rotation=45)\n",
    "                            plt.yticks(rotation=0)\n",
    "                            \n",
    "                            # Add interpretation note\n",
    "                            plt.figtext(0.5, 0.01, \n",
    "                                       \"Green = Better protection, Red = Worse protection compared to baseline\", \n",
    "                                       ha='center', fontsize=12)\n",
    "                            \n",
    "                            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                            plt.show()\n",
    "                        else:\n",
    "                            print(f\"‚ö†Ô∏è Insufficient diversity for heatmap: {unique_boundaries} boundaries, {unique_subtypes} subtypes\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Insufficient data for visualization: only {len(effectiveness_df)} combinations found\")\n",
    "                        print(\"Showing basic statistics instead:\")\n",
    "                        print(effectiveness_df.describe())\n",
    "                \n",
    "                else:\n",
    "                    print(\"‚ùå No relative effectiveness data could be calculated\")\n",
    "                    print(\"This might happen if:\")\n",
    "                    print(\"- All boundaries have the same performance\")\n",
    "                    print(\"- No baseline boundary ('none') exists\")\n",
    "                    print(\"- Data is too sparse\")\n",
    "                    \n",
    "                    # Show basic statistics as fallback\n",
    "                    print(\"\\nüìä Basic Statistics by Boundary:\")\n",
    "                    basic_stats = results_df.groupby('boundary')['attack_success'].agg(['mean', 'std', 'count'])\n",
    "                    basic_stats['mean'] = basic_stats['mean'] * 100  # Convert to percentage\n",
    "                    basic_stats['std'] = basic_stats['std'] * 100   # Convert to percentage\n",
    "                    print(basic_stats.round(2))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in subtype analysis: {e}\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            \n",
    "            # Show debugging information\n",
    "            print(\"\\nüîç Debug Information:\")\n",
    "            print(\"Sample of data:\")\n",
    "            print(results_df[required_cols].head())\n",
    "            print(\"\\nData types:\")\n",
    "            print(results_df[required_cols].dtypes)\n",
    "            print(f\"\\nNull values:\")\n",
    "            print(results_df[required_cols].isnull().sum())\n",
    "            \n",
    "            # Try a very basic analysis as fallback\n",
    "            print(\"\\nüîß Attempting basic fallback analysis...\")\n",
    "            try:\n",
    "                simple_stats = results_df.groupby(['attack_type', 'boundary'])['attack_success'].mean() * 100\n",
    "                print(\"Basic success rates by attack type and boundary:\")\n",
    "                print(simple_stats.round(1))\n",
    "                \n",
    "                # Simple visualization\n",
    "                if len(simple_stats) > 0:\n",
    "                    simple_df = simple_stats.reset_index()\n",
    "                    simple_df.columns = ['Attack Type', 'Boundary', 'Success Rate (%)']\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    sns.barplot(data=simple_df, x='Attack Type', y='Success Rate (%)', hue='Boundary')\n",
    "                    plt.title('Basic Attack Success Rates', fontsize=16)\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"‚ùå Even fallback analysis failed: {fallback_error}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results_df available for subtype effectiveness analysis\")\n",
    "    print(\"Please run experiments first using: python src/main.py --quick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Recommendations\n",
    "\n",
    "Based on our analysis, let's summarize the key findings and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Findings and Recommendations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "findings = [\n",
    "    \"Finding 1: Hybrid boundaries generally provide the strongest protection across all modalities, but at the highest implementation cost.\",\n",
    "    \"Finding 2: Token-based boundaries are more effective for structured data attacks compared to semantic boundaries.\",\n",
    "    \"Finding 3: Visual attacks tend to be the most successful against all boundary types, suggesting a specific vulnerability in cross-modal transfer.\",\n",
    "    \"Finding 4: Model architecture significantly impacts vulnerability patterns, with different models showing distinct weaknesses against specific attack types.\"\n",
    "]\n",
    "\n",
    "recommendations = [\n",
    "    \"Recommendation 1: For highest security requirements, implement hybrid boundaries despite the added complexity.\",\n",
    "    \"Recommendation 2: For simpler deployments with good protection, token-based boundaries offer the best security-to-complexity ratio.\",\n",
    "    \"Recommendation 3: Strengthen visual modality protection specifically, as it shows the highest vulnerability across boundary types.\",\n",
    "    \"Recommendation 4: Match boundary mechanisms to model architecture, as different models show varying degrees of protection from the same boundary type.\"\n",
    "]\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i}. {finding}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, recommendation in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Security-Complexity Frontier Analysis\n",
    "\n",
    "Let's analyze the relationship between security effectiveness and implementation complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    # Define complexity scores more precisely\n",
    "    complexity_scores = {\n",
    "        'none': {'score': 0, 'implementation_hours': 0, 'prompt_overhead': 0},\n",
    "        'token': {'score': 2, 'implementation_hours': 4, 'prompt_overhead': 15},\n",
    "        'semantic': {'score': 3, 'implementation_hours': 8, 'prompt_overhead': 25},\n",
    "        'hybrid': {'score': 4, 'implementation_hours': 16, 'prompt_overhead': 40}\n",
    "    }\n",
    "    \n",
    "    # Create data for the frontier analysis\n",
    "    frontier_data = []\n",
    "    \n",
    "    for boundary, stats in complexity_scores.items():\n",
    "        # Get the protection rate for this boundary\n",
    "        boundary_df = results_df[results_df['boundary'] == boundary]\n",
    "        \n",
    "        if len(boundary_df) > 0:\n",
    "            protection_rate = 100 - (boundary_df['attack_success'].mean() * 100)\n",
    "            \n",
    "            frontier_data.append({\n",
    "                'Boundary': boundary,\n",
    "                'Protection Rate (%)': protection_rate,\n",
    "                'Implementation Time (hours)': stats['implementation_hours'],\n",
    "                'Prompt Overhead (%)': stats['prompt_overhead'],\n",
    "                'Complexity Score': stats['score']\n",
    "            })\n",
    "    \n",
    "    frontier_df = pd.DataFrame(frontier_data)\n",
    "    \n",
    "    # Create frontier visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create scatter plot with size based on protection rate\n",
    "    scatter = plt.scatter(\n",
    "        frontier_df['Implementation Time (hours)'],\n",
    "        frontier_df['Prompt Overhead (%)'],\n",
    "        s=frontier_df['Protection Rate (%)'] * 5,  # Size based on protection\n",
    "        alpha=0.7,\n",
    "        c=frontier_df['Complexity Score'],  # Color based on complexity\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, row in frontier_df.iterrows():\n",
    "        label = f\"{row['Boundary']}\\n{row['Protection Rate (%)']:.1f}%\"\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            (row['Implementation Time (hours)'], row['Prompt Overhead (%)']),\n",
    "            xytext=(10, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    # Add color bar legend\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Complexity Score', fontsize=12)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Security-Complexity Frontier Analysis', fontsize=16)\n",
    "    plt.xlabel('Implementation Time (hours)', fontsize=14)\n",
    "    plt.ylabel('Prompt Overhead (%)', fontsize=14)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Add note about the plot\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        \"Bubble size represents protection rate (larger = better). Lower implementation time and prompt overhead is better.\",\n",
    "        ha='center',\n",
    "        fontsize=12\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the frontier data table\n",
    "    display(frontier_df.sort_values('Protection Rate (%)', ascending=False).round(2))\n",
    "    \n",
    "    # Calculate efficiency ratio (protection per complexity unit)\n",
    "    frontier_df['Efficiency Ratio'] = frontier_df['Protection Rate (%)'] / frontier_df['Complexity Score'].replace(0, 0.1)\n",
    "    \n",
    "    # Plot efficiency ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Boundary', y='Efficiency Ratio', data=frontier_df)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Protection Efficiency by Boundary Type', fontsize=16)\n",
    "    plt.xlabel('Boundary Type', fontsize=14)\n",
    "    plt.ylabel('Efficiency Ratio (Protection / Complexity)', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add note about the plot\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        \"Higher efficiency ratio indicates better protection for implementation effort\",\n",
    "        ha='center',\n",
    "        fontsize=12\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "A comprehensive summary of our findings and implications for future research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Conclusion\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"This comprehensive analysis of multi-modal context boundary mechanisms has yielded several important insights:\n",
    "\n",
    "1. **Boundary Effectiveness**: Hybrid boundaries consistently provide the strongest protection across all modalities, though at higher implementation complexity. Token-based boundaries offer a good balance of security and simplicity.\n",
    "\n",
    "2. **Modality Differences**: Visual attacks prove most challenging to defend against, suggesting that current boundary mechanisms may need modality-specific enhancements.\n",
    "\n",
    "3. **Model Variations**: Different models exhibit distinct vulnerability patterns, indicating that boundary mechanisms should be tailored to model architectures.\n",
    "\n",
    "4. **Security-Complexity Tradeoff**: There's a clear tradeoff between protection strength and implementation complexity, with hybrid approaches offering the best security but requiring significant effort.\n",
    "\n",
    "5. **Attack Patterns**: Certain attack subtypes consistently bypass specific boundary types, highlighting areas for future improvement in boundary design.\n",
    "\n",
    "These findings provide actionable guidance for implementing context boundaries in multi-modal LLM applications. Future research should focus on:\n",
    "- Developing more efficient hybrid boundary implementations\n",
    "- Creating modality-specific boundary enhancements\n",
    "- Exploring model-adaptive boundary mechanisms\n",
    "- Investigating new boundary paradigms beyond current approaches\n",
    "\n",
    "The security-complexity frontier analysis suggests that token-based boundaries currently offer the best practical balance for most applications, while hybrid approaches may be warranted for high-security scenarios.\"\"\")\n",
    "\n",
    "# Final summary statistics with robust error handling\n",
    "if results_df is not None:\n",
    "    print(\"\\nüîç Generating Final Summary Statistics...\")\n",
    "    print(f\"Data available: {len(results_df)} experiments\")\n",
    "    \n",
    "    # Check what boundary types we actually have\n",
    "    available_boundaries = results_df['boundary'].unique()\n",
    "    print(f\"Available boundary types: {list(available_boundaries)}\")\n",
    "    \n",
    "    # Calculate overall protection rates (1 - attack success rate)\n",
    "    boundary_stats = results_df.groupby('boundary')['attack_success'].agg(['mean', 'std', 'count'])\n",
    "    boundary_stats['protection_rate'] = (1 - boundary_stats['mean']) * 100\n",
    "    boundary_stats['attack_success_rate'] = boundary_stats['mean'] * 100\n",
    "    boundary_stats['protection_std'] = boundary_stats['std'] * 100\n",
    "    \n",
    "    # Sort by protection rate (highest first)\n",
    "    boundary_stats = boundary_stats.sort_values('protection_rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nBoundary Performance Summary:\")\n",
    "    display_stats = boundary_stats[['protection_rate', 'attack_success_rate', 'protection_std', 'count']].copy()\n",
    "    display_stats.columns = ['Protection Rate (%)', 'Attack Success Rate (%)', 'Std Dev (%)', 'Sample Count']\n",
    "    display(display_stats.round(2))\n",
    "    \n",
    "    # Create visualization regardless of whether 'none' boundary exists\n",
    "    print(\"\\nüìä Creating summary visualization...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if we have a 'none' baseline for comparison\n",
    "        has_none_baseline = 'none' in available_boundaries\n",
    "        \n",
    "        if has_none_baseline:\n",
    "            print(\"‚úÖ Found 'none' baseline for relative comparison\")\n",
    "            \n",
    "            # Calculate relative improvement over no boundary\n",
    "            baseline_protection = boundary_stats.loc['none', 'protection_rate']\n",
    "            baseline_success = boundary_stats.loc['none', 'attack_success_rate']\n",
    "            \n",
    "            improvements = {}\n",
    "            \n",
    "            for boundary in available_boundaries:\n",
    "                if boundary != 'none':\n",
    "                    current_protection = boundary_stats.loc[boundary, 'protection_rate']\n",
    "                    current_success = boundary_stats.loc[boundary, 'attack_success_rate']\n",
    "                    \n",
    "                    absolute_improvement = current_protection - baseline_protection\n",
    "                    \n",
    "                    # Calculate relative improvement in protection\n",
    "                    if baseline_protection < 100:\n",
    "                        relative_improvement = (absolute_improvement / (100 - baseline_protection)) * 100\n",
    "                    else:\n",
    "                        relative_improvement = 0\n",
    "                    \n",
    "                    improvements[boundary] = {\n",
    "                        'Protection Rate (%)': current_protection,\n",
    "                        'Absolute Improvement (pp)': absolute_improvement,\n",
    "                        'Relative Improvement (%)': relative_improvement,\n",
    "                        'Sample Count': boundary_stats.loc[boundary, 'count']\n",
    "                    }\n",
    "            \n",
    "            if improvements:\n",
    "                improvements_df = pd.DataFrame(improvements).T\n",
    "                improvements_df = improvements_df.sort_values('Protection Rate (%)', ascending=False)\n",
    "                \n",
    "                print(f\"\\nBoundary Effectiveness Summary (compared to '{baseline_protection:.1f}%' baseline):\")\n",
    "                display(improvements_df.round(2))\n",
    "                \n",
    "                # Create dual-axis visualization\n",
    "                fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "                \n",
    "                x = np.arange(len(improvements_df))\n",
    "                width = 0.35\n",
    "                \n",
    "                # Plot protection rate on left axis\n",
    "                bars1 = ax1.bar(x - width/2, improvements_df['Protection Rate (%)'], \n",
    "                               width, label='Protection Rate (%)', color='skyblue', alpha=0.8)\n",
    "                \n",
    "                # Create second y-axis for relative improvement\n",
    "                ax2 = ax1.twinx()\n",
    "                bars2 = ax2.bar(x + width/2, improvements_df['Relative Improvement (%)'], \n",
    "                               width, label='Relative Improvement (%)', color='lightgreen', alpha=0.8)\n",
    "                \n",
    "                # Customize axes\n",
    "                ax1.set_xlabel('Boundary Type', fontsize=14)\n",
    "                ax1.set_ylabel('Protection Rate (%)', fontsize=14, color='blue')\n",
    "                ax2.set_ylabel('Relative Improvement (%)', fontsize=14, color='green')\n",
    "                \n",
    "                # Set x-tick labels\n",
    "                ax1.set_xticks(x)\n",
    "                ax1.set_xticklabels(improvements_df.index, rotation=45, ha='right')\n",
    "                \n",
    "                # Add title\n",
    "                plt.title('Boundary Effectiveness Summary\\n(Compared to No Boundary Baseline)', fontsize=16)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, value in zip(bars1, improvements_df['Protection Rate (%)']):\n",
    "                    height = bar.get_height()\n",
    "                    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "                \n",
    "                for bar, value in zip(bars2, improvements_df['Relative Improvement (%)']):\n",
    "                    height = bar.get_height()\n",
    "                    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "                \n",
    "                # Add legends\n",
    "                ax1.legend(loc='upper left')\n",
    "                ax2.legend(loc='upper right')\n",
    "                \n",
    "                # Add grid\n",
    "                ax1.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No boundary improvements to display (only baseline found)\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No 'none' baseline found - showing absolute performance comparison\")\n",
    "            \n",
    "            # Create simple comparison without baseline\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Left plot: Protection rates\n",
    "            boundary_stats['protection_rate'].plot(kind='bar', ax=ax1, color='skyblue', alpha=0.8)\n",
    "            ax1.set_title('Protection Rate by Boundary Type', fontsize=14)\n",
    "            ax1.set_ylabel('Protection Rate (%)', fontsize=12)\n",
    "            ax1.set_xlabel('Boundary Type', fontsize=12)\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            ax1.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (boundary, row) in enumerate(boundary_stats.iterrows()):\n",
    "                ax1.text(i, row['protection_rate'] + 1, f'{row[\"protection_rate\"]:.1f}%', \n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Right plot: Sample counts with protection rate as color\n",
    "            colors = plt.cm.viridis(boundary_stats['protection_rate'] / boundary_stats['protection_rate'].max())\n",
    "            bars = ax2.bar(range(len(boundary_stats)), boundary_stats['count'], color=colors, alpha=0.8)\n",
    "            \n",
    "            ax2.set_title('Sample Count by Boundary Type', fontsize=14)\n",
    "            ax2.set_ylabel('Number of Experiments', fontsize=12)\n",
    "            ax2.set_xlabel('Boundary Type', fontsize=12)\n",
    "            ax2.set_xticks(range(len(boundary_stats)))\n",
    "            ax2.set_xticklabels(boundary_stats.index, rotation=45, ha='right')\n",
    "            ax2.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (boundary, row) in enumerate(boundary_stats.iterrows()):\n",
    "                ax2.text(i, row['count'] + 0.5, f'{int(row[\"count\"])}', \n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Add colorbar for protection rate\n",
    "            sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, \n",
    "                                     norm=plt.Normalize(vmin=boundary_stats['protection_rate'].min(), \n",
    "                                                       vmax=boundary_stats['protection_rate'].max()))\n",
    "            sm.set_array([])\n",
    "            cbar = plt.colorbar(sm, ax=ax2)\n",
    "            cbar.set_label('Protection Rate (%)', fontsize=12)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Additional summary visualization: Performance ranking\n",
    "        print(\"\\nüìà Creating performance ranking visualization...\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create ranking plot\n",
    "        ranking_data = boundary_stats.reset_index()\n",
    "        ranking_data['rank'] = range(1, len(ranking_data) + 1)\n",
    "        \n",
    "        # Create horizontal bar plot for better readability\n",
    "        bars = plt.barh(ranking_data['boundary'], ranking_data['protection_rate'], \n",
    "                       color=plt.cm.RdYlGn(ranking_data['protection_rate'] / 100), alpha=0.8)\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Boundary Protection Rate Ranking', fontsize=16)\n",
    "        plt.xlabel('Protection Rate (%)', fontsize=14)\n",
    "        plt.ylabel('Boundary Type', fontsize=14)\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, ranking_data['protection_rate']):\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{value:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "        \n",
    "        # Add ranking numbers\n",
    "        for i, (_, row) in enumerate(ranking_data.iterrows()):\n",
    "            plt.text(5, i, f'#{row[\"rank\"]}', ha='left', va='center', \n",
    "                    fontsize=12, fontweight='bold', color='white',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Final insights\n",
    "        print(\"\\nüéØ Key Insights from Final Analysis:\")\n",
    "        \n",
    "        best_boundary = boundary_stats.index[0]\n",
    "        worst_boundary = boundary_stats.index[-1]\n",
    "        best_protection = boundary_stats.iloc[0]['protection_rate']\n",
    "        worst_protection = boundary_stats.iloc[-1]['protection_rate']\n",
    "        protection_gap = best_protection - worst_protection\n",
    "        \n",
    "        print(f\"‚Ä¢ Best performing boundary: {best_boundary} ({best_protection:.1f}% protection)\")\n",
    "        print(f\"‚Ä¢ Worst performing boundary: {worst_boundary} ({worst_protection:.1f}% protection)\")\n",
    "        print(f\"‚Ä¢ Performance gap: {protection_gap:.1f} percentage points\")\n",
    "        \n",
    "        if len(boundary_stats) > 1:\n",
    "            median_protection = boundary_stats['protection_rate'].median()\n",
    "            print(f\"‚Ä¢ Median protection rate: {median_protection:.1f}%\")\n",
    "            \n",
    "            # Statistical significance note\n",
    "            total_experiments = boundary_stats['count'].sum()\n",
    "            print(f\"‚Ä¢ Total experiments analyzed: {total_experiments}\")\n",
    "            print(f\"‚Ä¢ Average experiments per boundary: {total_experiments / len(boundary_stats):.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating summary visualization: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        \n",
    "        # Fallback: simple table display\n",
    "        print(\"\\nüìã Fallback: Basic Statistics Table\")\n",
    "        print(\"Boundary Performance (Protection Rate %):\")\n",
    "        simple_stats = results_df.groupby('boundary')['attack_success'].mean()\n",
    "        simple_protection = (1 - simple_stats) * 100\n",
    "        simple_protection_sorted = simple_protection.sort_values(ascending=False)\n",
    "        \n",
    "        for boundary, rate in simple_protection_sorted.items():\n",
    "            print(f\"  {boundary}: {rate:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results data available for final summary\")\n",
    "    print(\"Please run experiments first using: python src/main.py --quick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Visualizations (Optional)\n",
    "\n",
    "This section allows you to export key visualizations for your paper or presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export key visualizations for the paper\n",
    "if results_df is not None:\n",
    "    # Create output directory for figures\n",
    "    os.makedirs('../docs/figures', exist_ok=True)\n",
    "    \n",
    "    print(\"Saving key visualizations for paper...\")\n",
    "    print(\"To export figures, uncomment and customize the code below:\")\n",
    "    print(\"\"\"\n",
    "    # Example export code:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='boundary', y='Protection Rate (%)', data=improvements_df)\n",
    "    plt.title('Boundary Protection Effectiveness', fontsize=16)\n",
    "    plt.savefig('../docs/figures/boundary_effectiveness.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Export modality-specific effectiveness\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=modal_df, x='Modality', y='Effectiveness (%)', hue='Boundary')\n",
    "    plt.title('Cross-Modal Protection by Boundary Type', fontsize=16)\n",
    "    plt.savefig('../docs/figures/cross_modal_effectiveness.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Export security-complexity frontier\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        frontier_df['Implementation Time (hours)'],\n",
    "        frontier_df['Prompt Overhead (%)'],\n",
    "        s=frontier_df['Protection Rate (%)'] * 5,\n",
    "        alpha=0.7,\n",
    "        c=frontier_df['Complexity Score'],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.title('Security-Complexity Frontier', fontsize=16)\n",
    "    plt.savefig('../docs/figures/security_complexity_frontier.png', dpi=300, bbox_inches='tight')\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nVisualization paths when exported:\\n - '../docs/figures/boundary_effectiveness.png'\\n - '../docs/figures/cross_modal_effectiveness.png'\\n - '../docs/figures/security_complexity_frontier.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Future Work\n",
    "\n",
    "Suggestions for extending this research and addressing limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Future Work\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "future_work = [\n",
    "    \"1. **Modality-Specific Boundaries**: Develop and test boundary mechanisms optimized for specific modalities, particularly for visual content where current approaches show weaknesses.\",\n",
    "    \n",
    "    \"2. **Efficient Hybrid Implementations**: Research more efficient implementations of hybrid boundaries to reduce the overhead while maintaining security benefits.\",\n",
    "    \n",
    "    \"3. **Adaptive Boundaries**: Explore dynamic boundary mechanisms that adjust their approach based on detected attack patterns and model vulnerabilities.\",\n",
    "    \n",
    "    \"4. **Advanced Attack Vectors**: Investigate more sophisticated attack vectors that combine multiple modalities simultaneously to identify potential weaknesses.\",\n",
    "    \n",
    "    \"5. **Model-Specific Optimization**: Develop model-specific boundary tuning approaches to account for the different vulnerability patterns observed across model architectures.\",\n",
    "    \n",
    "    \"6. **Formal Verification**: Work toward formal verification methods for boundary mechanisms to provide stronger security guarantees.\",\n",
    "    \n",
    "    \"7. **Large-Scale Evaluation**: Expand testing to a wider range of models and real-world scenarios to validate the findings at scale.\"\n",
    "]\n",
    "\n",
    "for item in future_work:\n",
    "    print(item)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
